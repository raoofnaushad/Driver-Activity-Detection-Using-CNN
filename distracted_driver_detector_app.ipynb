{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone Project\n",
    "\n",
    "## Project: Machine Learning Model for Distracted Driver Detection \n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "The notebook is broken into separate steps as shown below.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Create a CNN to Classify Driver Images (from Scratch)\n",
    "* [Step 2](#step2): Train a CNN with Transfer Learning - Part1 (Using the bottleneck features)\n",
    "* [Step 3](#step3): Train a CNN with Transfer Learning - Part2 (Using Fine-tuning)\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Driver Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of driver images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 150 total images.\n",
      "\n",
      "There are 80 training images.\n",
      "There are 10 total training categories.\n",
      "There are 20 validation images.\n",
      "There are 50 test images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# define function to load datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), 10)\n",
    "    return files, targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('images/train')\n",
    "test_files, test_targets = load_dataset('images/test')\n",
    "#valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "\n",
    "\n",
    "# load list of names\n",
    "names = [item[17:19] for item in sorted(glob(\"images/train/*/\"))]\n",
    "\n",
    "# break training set into training and validation sets\n",
    "#(train_files, valid_files) = train_files[:18000], train_files[18000:]\n",
    "#(train_targets, valid_targets) = train_targets[:18000], train_targets[18000:]\n",
    "train_files, valid_files, train_targets, valid_targets = train_test_split(train_files, train_targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# print statistics about the dataset\n",
    "\n",
    "print('There are %s total images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training images.' % len(train_files))\n",
    "print('There are %d total training categories.' % len(names))\n",
    "print('There are %d validation images.' % len(valid_files))\n",
    "print('There are %d test images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    c0\n",
      "1    c0\n",
      "2    c0\n",
      "Name: classname, dtype: object\n",
      "count     22424\n",
      "unique       10\n",
      "top          c0\n",
      "freq       2489\n",
      "Name: classname, dtype: object\n",
      "\n",
      " Image Counts\n",
      "c1    2267\n",
      "c2    2317\n",
      "c9    2129\n",
      "c8    1911\n",
      "c0    2489\n",
      "c5    2312\n",
      "c3    2346\n",
      "c7    2002\n",
      "c4    2326\n",
      "c6    2325\n",
      "Name: classname, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"driver_imgs_list.csv\",header='infer')\n",
    "print(df['classname'].head(3))\n",
    "print(df.iloc[:,1].describe())\n",
    "print(\"\\n Image Counts\")\n",
    "print(df['classname'].value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKoAAAHDCAYAAAAEI/ImAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nOzde5htV1kn6t9HQggBEjYhXERCCKgR8NZu2oZWEGmEcIAgiIh6xGvaYyNoFAWFhwh2A9qANhG59CNoC3IAuQgSAkECCIrsiKJAMEcNF7mE4IYYdkggfOePOYusFLV3VSVVtQa73vd51rNqzTnmXN+qsVfV2r8aY8zq7gAAAADAsl1v2QUAAAAAQCKoAgAAAGAQgioAAAAAhiCoAgAAAGAIgioAAAAAhiCoAgAAAGAIgioAgF2mqs6sqq6q85ZdCwDAIkEVAHDYWwhm1rodqKoLq+oPquruy651M6rqBlX1E1X18qr656r696q6oqo+XlVvrqonVNXtl13ntTX325lVddKyawEAdsaRyy4AAGCHfXLh6+sluVmSO863H62qX+/uM5dR2GZU1QOSPC/J1yxsviLJgSS3THKrJN+T5Myqen53/+zOV3mdPWm+Py/JRcsrAwDYKUZUAQC7SnffauF2iyQ3SPKdSc6fmzxp9JFVVfVfk7wmU0j1kST/LcmJ3X10d+/J9JrukeR3k3wxyQ8tq1YAgM0QVAEAu1p3X9Xd70jy4IXNpy2rnvVU1X9Oclamz3FvS/JN3f2c7v7ISpvu/kJ3v727H5Xk65P8xXKqBQDYHEEVAECS7v5okk/PD2+8en9VXb+qHlRVz6+qffM6UFdW1cVVdU5VPaKqaq1zV9V3r6yJNT/+tqp6cVV9tKq+sMlFzZ+RafmGi5M8tLs/u87r+nCSBx2qTVXdu6r+rKo+VVWfr6oPVNWTqurog7Q/Zn69f1hVfzsfd0VVfayqXl1Vpx7iuX5s/l5cND++13zMx6vqqqp60XzrhcPesmpdsYsO9XoAgK9e1qgCAEhSVbdJcvz88INrNPnPmabbrbg0yeeTnJDke+fb91XVD3b3lw7xPA9N8sdJrj+f44ubqPGuSb5jfvjs7r5kI8etU89jkzx9fvjZJEclOSXJmUnuWVX36e6rVh32A0leuHL6XP06bp1pNNppVfWM7v6ldV7PY5I8K0nNz73yPJ/NtJbYLefH+5NcuXDopw51XgDgq5cRVQDArlZVR1TV3ZK8at50cZI/XKPpgUyLl98nyXHdfVx3H5sp3HpMprDmYUketc5TvijJm5J843yOGyb56Q2We++Fr1910FYb9y1JnjbfbjGvb3XTJE+e998rySPXOG5/kv+ZaW2vG3f3Tbv7RpnWzHpSki8k+cWqOtRIrltmGh32B5nW17ppkhsmeUp3P6a7b7XQ9iGr1ha767V9wQDA2IyoAgB2lar6xMLDlav+HZEpaHpxkl/r7s+sPq67/zrJX6+x/d+S/K+q+liSlyd5dJL/dYgS3p/kQYujlLr7wg2Wf+f5/ookH9jgMYdy0yTXuMphd1+aaUH5uyR5SJJHJPn9xYO6+zW55uiyle0fT/LkqjqQ5LcyfS/+9CDPfXSSV3b3jy8cf1WSf7ouLwgA+OpmRBUAsNvccuF2QqaQKkmOSXJcrp5utll/Nt/foapudYh2v7XGVLqNWpma+JlDTefbhCsyjYxay0oQ9c3X4rwr34u7VdURh2j31GtxbgDgMCaoAgB2le6uxVum6WbflmkK2gOSvK2qHrzWsVV1k6p6bFW9dV5E/cqFRdIPLDT92kOU8I6tei1b4H3dfdlB9n1svr/ZWjur6pZV9etV9ZdV9emq+uLC9+L9c7Njkuw5yPkvT/I317pyAOCwZOofALCrdffnk/xtkp+qqpsl+b4kL6qqE+dpcEmSqvr6JG/ONUOoA0k+k2RldNPKaKwbHeIpL74O5a5clfCmVXW9LRhV9e+H2LeyyPtXfF6c1/R6faapgysuy/T96Eyj1G4+b79RkrUWff/0Fo0KAwAOI0ZUAQBc7QXz/XFJ7r9q3wszhVQXZVo0/fjuvlF332Je+Ps2C23rYE9wHab9Jcn75vsbJPnG63Cea62qjsx01cKbZgr47p/k2O6+SXffcv5e/KfFQw5yquvyfQAADlNGVAEAXO1DC1/ffuWLqrptkrvPDx/R3X+1xrGHWpdqq7x54evvy9XB1U66W5LbZQqaHtDd/7pGm534XgAAhyEjqgAArrY4re9zC1/fduHr9xzk2P+y9eVcU3e/O1dfefBRVXXzQ7VfUVVb+Zlv5XvxqYOEVMnWfS96vj/oCDUA4PAiqAIAuNoPLXy9b+Hrzy58/S2rD6qqmyR5wnYVtcovZRrNdMskf1JVxx2qcVV9bZJXb+Hzr3wvbllVX3GFxPn5Hr1Fz7WyRthND9kKADhsCKoAgF2vqm5VVb+R5JHzpr9K8pcLTT6Q5MPz179fVd++cOzdkpyXg1/dbkt199uTPCbTaKN7JHlvVf3sHBCt1HT9qrp7Vf12kn+c222Vv8g02qySvGxeZD5VdURV3TfT96IPfvim/MN8/8NVdcwWnRMAGJg1qgCAXaWqPrFq09GZFk9f8fdJHtrdXw5buvtLVfXfkrwqyZ2T7KuqA/PuYzIFN6clOXfbCl/Q3b9bVR9N8ntJTkzyu0l+t6o+n+TyTCOQVqbLfTHJ87bwuT9bVb80P/c9knywqi7L9Lny6ExX+PvxJH+6BU/33CT/OclDkzyoqi7O9Ho+2t3fuQXnBwAGI6gCAHab1dPVvpDkE0n+Lskrkvxhd1+5+qDufl1V3SPJr2UKT46Zj3tzkqd39werdm4ppe5+TVWdk+RHkpya5D8kOSHJjZJcnGk00p8n+T/d/ZEtfu7nVtWHkzw2yd5Mnyn/NcnrkzwtyVFb9Dx/NH9P/2uSb0py65gRAACHtVr4YyEAAAAALI2/SAEAAAAwBEEVAAAAAEMQVAEAAAAwBEEVAAAAAEMQVAEAAAAwhCOXXcDIbn7zm/dJJ5207DIAAAAADhvnn3/+Jd19wlr7BFWHcNJJJ2Xfvn3LLgMAAADgsFFVHzrYPlP/AAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIexoUFVVD6uqP62qf62qy6rq/Kp6xKo251VVr3E7elW721TVq6rq36vqkqo6q6qOWeM5f7qqLqyqz8/Pd+/tfp0AAAAAbN6RO/x8ZyT5lyS/kOSSJPdP8pKqunl3P3uh3VuS/OqqY69Y+aKqrp/knCRXJvnBJDdN8sz5/kcW2j0iyXOTnJnkL5L8eJLXVdVdu/sftvSVAQAAAHCd7HRQ9cDuvmTh8Z9X1ddkCrAWg6p/6+6/OsR5vj/JNya5Y3f/S5JU1ReSvLSqfr27L5zbnZnkD7r7KXObtyb5tiSPy0KgBQAAAMDy7ejUv1Uh1Yr3JPmaTZ7q1CTvXgmpZq/ONMLqfklSVScn+fokL1t4/i8lefl8PAAAAAADGWEx9bsl+cdV2763qg7Mt3Oq6ptX7T8lyQWLG7r7yiT/NO/Lwv012iX5QJKbVdUJ1710AAAAALbKUoOqeWHzByd5xsLmtyZ5TJL7Jjk9yYlJ3l5VJy202ZPkM2uccv+8Lwv3q9vtX7UfAAAAgAHs9BpVXzYHTy9J8pruftHK9u5+0kKzt1fVuZlGRf38fNvuuk7PFJDlxBNP3O6nAwAAAGC2lBFVVXWzJGcn+VCSHz5U2+7+RJJ3JPkPC5v3JzlujeZ7cvWIqZX71e32rNq/+vme3917u3vvCSeYHQgAAACwU3Z8RFVVHZPkdUmOSvKA7j6wgcN6vq24IFevQbVy3qOSnJzkuQttMrf70ELTUzJdVfBTm68eALguqpZdwZi6128DALAb7OiIqqo6MtNV974uyf26++INHHOrJN+Z5PyFzWcnuWtV3W5h24OS3CDJG5Kku/850yLtD1s41/Xmx2dft1cCAAAAwFbb6RFVz0ly/0yLpR9fVccv7HtPkm9I8tRMYdaHMi2k/vgkX0ry2wttX5Hk15K8sqqemGl637OSvKS7L1xod2aSP6qqizJNH3xkppDsh7b6hQEAAABw3ex0UPW98/3vrLHv9kk+naQyhVXHJ/n3JOcleXB3f3ilYXd/oarul+SsJC9LckWSlyZ57OIJu/uPq+rGSX4lyROTvC/TdMN/2MLXBAAAAMAW2NGgqrtP2kCz+2/wXB9N8uANtHtBkhds5JwAAAAALM9SrvoHAAAAAKsJqgAAAAAYgqAKAAAAgCEIqgAAAAAYgqAKAAAAgCEIqgAAAAAYgqAKAAAAgCEIqgAAAAAYgqAKAAAAgCEIqgAAAAAYgqAKAAAAgCEIqgAAAAAYgqAKAAAAgCEIqgAAAAAYgqAKAAAAgCEcuewCAACAjatadgVj6l52BQBsBSOqAAAAABiCoAoAAACAIZj6BwAAwK5iCu3aRppCq4/WNlIfbRdBFQzCD+K17YYfxAAAAEwEVQAAfJk/nKzNH04AYGdYowoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIbjq3y7hCj5rcwUfAAAAGIcRVQAAAAAMQVAFAAAAwBAEVQAAAAAMQVAFAAAAwBAEVQAAAAAMQVAFAAAAwBAEVQAAAAAMQVAFAAAAwBAEVQAAAAAMQVAFAAAAwBAEVQAAAAAMQVAFAAAAwBAEVQAAAAAMQVAFAAAAwBAEVQAAAAAMQVAFAAAAwBAEVQAAAAAMQVAFAAAAwBAEVQAAAAAM4chlFwDw1aJq2RWMqXvZFQAAAIcLI6oAAAAAGIKgCgAAAIAhCKoAAAAAGIKgCgAAAIAhCKoAAAAAGIKgCgAAAIAhCKoAAAAAGIKgCgAAAIAhCKoAAAAAGIKgCgAAAIAhCKoAAAAAGMKOBlVV9bCq+tOq+tequqyqzq+qR6zR7qer6sKq+vzc5t5rtLlNVb2qqv69qi6pqrOq6phrcy4AAAAAlm+nR1SdkeSyJL+Q5EFJ3pLkJVX1cysN5uDquUn+MMmpSd6X5HVVdZeFNtdPck6S2yX5wSSPSfKwJM9ffLKNnAsAAACAMVR379yTVd28uy9Zte0lSe7W3befH38wyTu6+yfmx9dL8ndJ/q67f2Te9ogkf5Tkjt39L/O2H0jy0iTf0N0XbvRch7J3797et2/fFrzy5atadgVj2sF//uvSR2vTR+MbqY8Yn/fR2kZ6H+mjtemj8Y3UR4zP+2htI72P9NHaRuqj66Kqzu/uvWvt29ERVatDqtl7knxNklTVyUm+PsnLFo75UpKXZxoRteLUJO9eCalmr05yZZL7bfJcAAAAAAxghMXU75bkH+evT5nvL1jV5gNJblZVJyy0u0ab7r4yyT8tnGOj5wIAAABgAEsNquaFzR+c5Bnzpj3z/WdWNd2/av+eNdqstNuzqu165wIAAABgAEcu64mr6qQkL0nymu5+0bLqWK2qTk9yepKceOKJS64GAAD4amNtnbUdLmvrANtrKSOqqupmSc5O8qEkP7ywa2W003GrDtmzav/+NdqstNu/qu1657qG7n5+d+/t7r0nnGB2IAAAAMBO2fGgqqqOSfK6JEcleUB3H1jYvbKe1CmrDjslyb9196cW2l2jTVUdleTkhXNs9FwAAAAADGBHg6qqOjLTVfe+Lsn9uvvixf3d/c+ZFlZ/2MIx15sfn73Q9Owkd62q2y1se1CSGyR5wybPBQAAAMAAdnqNquckuX+SxyQ5vqqOX9j3nu6+IsmZSf6oqi5K8o4kj8wUbP3QQttXJPm1JK+sqidmmt73rCQv6e4LF9pt5FwAAAAADGCng6rvne9/Z419t09yUXf/cVXdOMmvJHlikvdlmiL4DysNu/sLVXW/JGcleVmSK5K8NMljF0+4kXMBAAAAMIZql144qL179/a+ffuWXcaWcOWRtY30z18frU0fjW+kPmJ83kdrG+l9pI/Wpo/Gp4/Gp4/Gp4/GN1IfXRdVdX53711r31Ku+gcAAAAAqwmqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABjCkcsuAAC2issYr+1wuYwxAACHPyOqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABiCoAoAAACAIQiqAAAAABjCjgdVVXXHqnpeVb23qq6qqvPWaHNRVfWq2yfWaHenqnpzVR2oqo9V1ZOr6ohVbaqqfrWqPlJVl1fV26rqW7fxJQIAAABwLRy5hOe8c5L7J/mrJNc/RLuXJHn2wuMrF3dW1Z4k5yZ5f5LTktwhyTMyhW9PWGj6uCRPTPLYJBckOSPJuVV1l+7+ivALAAAAgOVYRlD12u5+TZJU1SuS3Pwg7T7e3X91iPP8TJIbJnlId1+a5E1VdWySM6vqN7v70qo6OlNQ9dTuPmt+zr9MclGSR+WagRYAAAAAS7TjU/+6+0tbdKpTk5wzh1QrXpopvLrn/PjuSY5N8rKF5/9cktfOxwMAAAAwiJEXU//Jqrqyqj5bVa+oqtut2n9Kpql8X9bdH05yYN630uaqJBeuOvYDC20AAAAAGMAypv5txGsyrWH10STfmORJSd5eVd/U3Z+d2+xJ8pk1jt0/71tpc1l3X7VGm2Oq6qjuvjIAAAAALN2QQVV3P2bh4dur6p1J/jbJjyf57e187qo6PcnpSXLiiSdu51MBAAAAsGDkqX9f1t3/kOSDSf7Dwub9SY5bo/meed9KmxtX1RFrtDmw1miq7n5+d+/t7r0nnHDCdS8eAAAAgA35qgiqZj3fVlyQVetMVdVtkxyTq9euuiDJEUnuuOpcX7G+FQAAAADL9VURVFXVXTKFS+cvbD47yX2r6iYL2x6e5PIkb50fvzPJpUketnCuY5I8cD4eAAAAgEHs+BpVc1B0//nhbZIcW1XfPz9+fZJ7JfmRJK9L8rFMAdUTknw4yYsWTvXcJI9O8sqqenqSk5OcmeSZ3X1pknT356vqaUmeWFX7M42iOiNTQPfsbXqJAAAAAFwLy1hM/RZJXr5q28rj2yf5yNzmt5PcNMmnk7whya+uBFBJ0t37q+reSc5K8tpMVwB8VqawatHTMgVTj09yfJJ9Se7T3Z/cupcEAAAAwHW140FVd1+UpNZpdu8Nnuv9Sb5nnTad5L/PNwAAAAAGteE1qqrqR6vq+IPsu1lV/ejWlQUAAADAbrOZxdRfmOQOB9l3+3k/AAAAAFwrmwmqDjVd7/hMV9cDAAAAgGvlkGtUVdVpSU5b2PTEqvrUqmZHJ/muJO/e4toAAAAA2EXWW0z9Fkm+aeHxHZLcalWbK5O8MclvbGFdAAAAAOwyhwyquvsFSV6QJFX1liT/T3dfsBOFAQAAALC7rDei6su6+17bWQgAAAAAu9uGg6okqaqvSfKAJF+baW2qRd3dv7JVhQEAAACwu2w4qKqq70vyx0mOSHJxprWpFnUSQRUAAAAA18pmRlT9j0yLpv9Yd//bNtUDAAAAwC61maDqtkl+TkgFAAAAwHa43ibavjPJN2xXIQAAAADsbpsZUXVGkhdX1WVJ3pTkM6sbdPeBrSoMAAAAgN1lM0HVe+f7F2ZaOH0tR1y3cgAAAADYrTYTVP1EDh5QAQAAAMB1suGgqrtftI11AAAAALDLbWYxdQAAAADYNhseUVVVn8o6U/+6+xbXuSIAAAAAdqXNrFH1u/nKoGpPknsnOTbJ729VUQAAAADsPptZo+rMtbZXVSV5WZIvbFFNAAAAAOxC13mNqu7uJP87yaOuezkAAAAA7FZbtZj6yUmO2qJzAQAAALALbWYx9Z9dY/NRSb4xyQ8neflWFQUAAADA7rOZxdTPWmPbFUk+muQ5SX59SyoCAAAAYFfazGLqWzVNEAAAAAC+gvAJAAAAgCFsKqiqqpOr6veq6u+r6l/n++dU1cnbVSAAAAAAu8NmFlP/9iRvSfL5JK9L8skkt0zy0CQ/XFX36u6/2ZYqAQAAADjsbWYx9f+Z5D1JTu3uAysbq+qYJK+f93/P1pYHAAAAwG6xmal//zHJby6GVEkyP/6fSb5jKwsDAAAAYHfZTFB1eZLjD7LvZpmmBAIAAADAtbKZoOrPkjytqr5zceP8+KlJXruVhQEAAACwu2xmjaozkrwmyVur6uIkFye5RaYF1d+Z5Be3vjwAAAAAdosNB1Xd/ekk31lV90ty1yS3TvLxJO/q7jduU30AAAAA7BKHnPpXVbeuqj+pqvuubOvuN3T3U7r7Z7v7KVOz+pOqusW2VwsAAADAYWu9Nap+KcnJSQ41YuqNSW4fU/8AAAAAuA7WC6oekOS53d0HazDve16S07ayMAAAAAB2l/WCqtslef8GzvOBJCdd52oAAAAA2LXWC6ouT3LsBs5z47ktAAAAAFwr6wVVf5PkQRs4z2lzWwAAAAC4VtYLqp6T5Cer6pEHa1BVP5rkx5OctZWFAQAAALC7HHmond39J1X1O0leWFWPSvKGJB9O0klOTHLfJHuTPKu7X7XdxQIAAABw+DpkUJUk3f2LVXVekp9P8ktJbjDvuiLJO5Kc1t2v27YKAQAAANgV1g2qkqS7X5vktVV1ZJLj582f7u4vbltlAAAAAOwqGwqqVszB1Ce3qRYAAAAAdrH1FlMHAAAAgB0hqAIAAABgCIIqAAAAAIYgqAIAAABgCIIqAAAAAIYgqAIAAABgCIIqAAAAAIYgqAIAAABgCIIqAAAAAIYgqAIAAABgCIIqAAAAAIYgqAIAAABgCIIqAAAAAIaw40FVVd2xqp5XVe+tqquq6rw12lRV/WpVfaSqLq+qt1XVt67R7k5V9eaqOlBVH6uqJ1fVEdfmXAAAAAAs1zJGVN05yf2TfDDJPx6kzeOSPDHJ05M8MMllSc6tqlutNKiqPUnOTdJJTkvy5CS/mOTXN3suAAAAAJZvGUHVa7v7tt39sCTvW72zqo7OFC49tbvP6u5zkzwsUyD1qIWmP5Pkhkke0t1v6u7nZgqpzqiqYzd5LgAAAACWbMeDqu7+0jpN7p7k2CQvWzjmc0lem+TUhXanJjmnuy9d2PbSTOHVPTd5LgAAAACWbMTF1E9JclWSC1dt/8C8b7HdBYsNuvvDSQ4stNvouQAAAABYshGDqj1JLuvuq1Zt35/kmKo6aqHdZ9Y4fv+8bzPnAgAAAGDJRgyqlqqqTq+qfVW171Of+tSyywEAAADYNUYMqvYnuXFVHbFq+54kB7r7yoV2x61x/J5532bO9WXd/fzu3tvde0844YRr/SIAAAAA2JwRg6oLkhyR5I6rtq9ek+qCrFpnqqpum+SYhXYbPRcAAAAASzZiUPXOJJcmedjKhqo6JskDk5y90O7sJPetqpssbHt4ksuTvHWT5wIAAABgyY7c6Secg6L7zw9vk+TYqvr++fHru/tAVT0tyROran+mkU9nZArVnr1wqucmeXSSV1bV05OcnOTMJM/s7kuTpLs/v8FzAQAAALBkOx5UJblFkpev2rby+PZJLkrytExh0uOTHJ9kX5L7dPcnVw7o7v1Vde8kZyV5baYrAD4rU1i1aN1zAQAAALB81d3LrmFYe/fu7X379i27jC1RtewKxjTSP399tDZ9ND59ND59ND59ND59ND59ND59ND59NL6R+ui6qKrzu3vvWvtGXKMKAAAAgF1IUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEARVAAAAAAxBUAUAAADAEIYMqqrqx6qq17j9zEKbqqpfraqPVNXlVfW2qvrWNc51p6p6c1UdqKqPVdWTq+qInX1FAAAAAKznyGUXsI7vSXL5wuN/Xvj6cUmemOSxSS5IckaSc6vqLt39iSSpqj1Jzk3y/iSnJblDkmdkCuiesO3VAwAAALBhowdV7+7uy1ZvrKqjMwVVT+3us+Ztf5nkoiSPytUh1M8kuWGSh3T3pUneVFXHJjmzqn5z3gYAAADAAIac+rcBd09ybJKXrWzo7s8leW2SUxfanZrknFWB1EszhVf33IE6AQAAANig0YOqf6qqL1bVB6vqvy5sPyXJVUkuXNX+A/O+xXYXLDbo7g8nObCqHQAAAABLNurUv49nWn/qr5MckeQHk0jLaFsAABNKSURBVDy3qo7p7mcl2ZPksu6+atVx+5McU1VHdfeVc7vPrHH+/fM+AAAAAAYxZFDV3eckOWdh09nzulRPqKrf2c7nrqrTk5yeJCeeeOJ2PhUAAAAAC0af+rfoFUluluSkTCOiblxVR6xqsyfJgXk0VeZ2x61xrj3zvq/Q3c/v7r3dvfeEE07YksIBAAAAWN9XU1DVC/cXZJoSeMdVbVavSXVBVq1FVVW3TXLMqnYAAAAALNlXU1D1/UkuSfKhJO9McmmSh63srKpjkjwwydkLx5yd5L5VdZOFbQ9PcnmSt253wQAAAABs3JBrVFXVn2RaSP29mUZOPXy+Pbq7v5Tk81X1tCRPrKr9mUZHnZEpeHv2wqmem+TRSV5ZVU9PcnKSM5M8s7sv3aGXAwAAAMAGDBlUJflgkp9IctskleT9SX60u//PQpunZQqmHp/k+CT7ktynuz+50qC791fVvZOcleS1ma4A+KxMYRUAAAAAA6nuXr/VLrV3797et2/fssvYElXLrmBMI/3z10dr00fj00fj00fj00fj00fj00fj00fj00fjG6mProuqOr+7966176tpjSoAAAAADmOCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGIKgCAAAAYAiCKgAAAACGsCuCqqq6U1W9uaoOVNXHqurJVXXEsusCAAAA4GpHLruA7VZVe5Kcm+T9SU5Lcockz8gU0j1hiaUBAAAAsOCwD6qS/EySGyZ5SHdfmuRNVXVskjOr6jfnbQAAAAAs2W6Y+ndqknNWBVIvzRRe3XM5JQEAAACw2m4Iqk5JcsHihu7+cJID8z4AAAAABrAbgqo9ST6zxvb98z4AAAAABrAb1qjalKo6Pcnp88PLquqDy6znMHXzJJcsu4gkqVp2BcPSR+PTR+PTR+PTR+PTR+PTR+PTR+PTR+PTR1vvdgfbsRuCqv1Jjltj+5553zV09/OTPH+7i9rNqmpfd+9ddh0cnD4anz4anz4anz4anz4anz4anz4anz4anz7aWbth6t8FWbUWVVXdNskxWbV2FQAAAADLsxuCqrOT3LeqbrKw7eFJLk/y1uWUBAAAAMBquyGoem6SK5K8sqr+y7wG1ZlJntndly61st3L1Mrx6aPx6aPx6aPx6aPx6aPx6aPx6aPx6aPx6aMdVN297Bq2XVXdKclZSe6W6QqA/zvJmd191VILAwAAAODLdkVQBQAAAMD4dsPUPwZTVUdV1W9V1dur6vKqkpYOpqruWlUvrKr/r6oOVNUHq+pJVXX0smvjalV156p649xHl1TV71XVjZddF1erqr1zH/3bfDu3qr5j2XVxtao6bv55t7+qPltVL66q45ddF5OqOqmqeo3bS5ddG5P5d9EbqupjVXVFVX24qv53Vd162bUxqaozD/I+6qp6/LLr45qq6npVtW/unwcsux6uqaoeUlXvnv8f++n559+Nll3X4ebIZRfArnRMkp9K8tdJ3pnke5ZbDmt4eJI7JHl6kguTfHOSp8z3D11iXcyq6rgkf57kHzP11/FJfjPJrZM8eImlMZuvMHtukr9J8n/Pmx+b5E1V9U3d/aGlFceilyX5+ky/l76U6efeq5N81zKL4iv8UpJ3LDy+ZFmF8BWOS/IvSf4wyceS3D7Jk5J8e1Xdtbu/uMziSDIte/KGVdsenORXMl14irH8VJKvXXYRfKWq+qlMSwr9ZqbPdHsy/V9WrrLFTP1jKaqqurur6lFJnt3dteyauFpV3by7L1m17fQkz0tykv9gL9/8F9DHJzmxuz8zb3tgkj9Nctfu3rfM+kiq6meS/G6Sm3X3Z+dtezL9B/tR3f17y6yPpKrulukPJvfs7rfN2/5jkncluU93n7vM+phGVGUKQR7Y3a9bbjVsVFXdJ8kbk3x7d//NsuvhK1XVnyU5ubu/cdm1cLX5c8I/JnlcpoDRz75BVNXNM/0+OqO7X7Dseg53pv6xbarqHlX1lqq6bJ5OcV5VfVuStIR0CAfro9Uh1ew98/3X7GSNu90h3kffmmTfSkg1e1OSTvJ/LaXYXeoQfXT9JF9M8rmF5pfN24TzO+gQfXRqkk+uhFRJ0t1/nemD6KnLqnc3OtRnBsawyT769Hx/1E7Vx8b7aJ7efJ8kf7zzVe5uG+ijp2QaPfrmJZW46x2ij35gbvIHy6xvtxBUsS2q6rsz/YD9QpJHZpqa9PYkt1liWSy4Fn10t0zTYv5pJ+pj3T46OsmVqw75YqY+8tfRHbJOH/1JkgNJnlFVt6iqWyR5VpL9SV6+lIJ3oXX66JQkF6xx2AfmfeyADf4+emFVXVVVH6+qZ1bVDXe+0t1rI300r6tzVFV9Q5KnJXl3pmUe2AGb/Fz30Ex/TBFU7aD1+qiqvjnJT2Sa6swSrNNH35Hkg0l+sqo+WlVfqKp3VdXdl1Xv4czUP7ZFVf1lpl+Adz3U6ClT/5Zno300t71VkvcmeX13/9gOlEcO3UdV9YwkP5Rp6t8X5m3fkeSvkrypu793p+vdjdZ7H1XVtyZ5Xa7+j8LHk5za3X+3c1Xubuu8j96U5HPd/eBV2/8o05QYHz53wDp9dOskv5ZpGtmlSb4707o6b+zu03a41F1rI58ZquoNSe47Pzw/yf27++IdKnHX2+Tnuj9Pclx3f/uOFEeSDX1meGuSd3X3L5v2vBzr/D46J8ndM/0u+uVMI0d/OcneJF/X3Z/c4XIPa0ZUseVquurBdyT5A1P8xrSZPqqqozItNnxZkl/YgfLIhvroBUlOSPLsqrpVVd05yXOSXJVpVBXbbL0+mv+D/fJM/2E7db6dn+TPqurEnax1t/L7aHzr9VF3f7y7H9Xdf9rd53X3mUnOSPKgqvqWHS53V9rE++jnkvynTBePuHGSs8vVgnfEJj/X3TrJPWM01Y7awGeGH0zyDUl+Y6drY7KB91Fl+tn2k9394u5+Q6aLElyV5FE7V+nuIKhiO+zJ9Eb++LIL4aA21EdVVZmu4nPnTH8Z3b8DtTE5ZB919wVJTk/yiLnNezNNsfjbJJ/YoRp3u/XeR4/N9Fe57+/uN8wfaB6a6QONYf07Y70+2p/pimVrHefn3c64Np8ZXjHfGw2yMzbUR919YXe/q7v/KNPIqm/LNPKX7beZ99EPzG3/322tiNUO2kdVdf0kv5XpqrPXq6qbJjl23n2jqrrJjlW5u23kM0MnOW9lQ3dfmumPkHfa7uJ2G0EV22F/phEdt152IRzURvvot5OcluS0ORhh56zbR939+0lumeSbMy1y/6gkd8w0/Y/tt14fnZLkfStTM5Oku69M8r4kd9j+8sj6fXRB1l6L6mBrV7H1rs1nhl51z/badB/NVwf+tyQnb1dRXMNm+ugHk/xFd39ke0tilUP10Y2SfG2SZ87t9idZWSLgpbn6gkZsr/XeRx/IFGStXrKmYjbDlhNUseW6+3OZLu39o/OIHAazkT6qqsdnCj5+pLv/YifrY+Pvo+7+fHf//Twv/kcy/Vx/2Q6VuattoI8+lOQu8/TZJElV3SDJXZJctCNF7nIb6KOzk9yqqr5zZUNV7c30n+uzd6bK3e1afmb4/vn+/O2pikXXpo/mBdWPz7TGDttso300r3v0n2La345bp48uS3KvVbdHzPt+NckP71Sdu9kG3kcra4Xda2VDVR2XaXSvtUe3mMXU2RZVdY8k5yb58yTPz3R59rsl2dfdr6uqUzP99eB+SX4yycPmQ989/xWObXaoPso03PjFSV6U5HmrDv2n7v7UzlW6e63TR2/LtMDw2zJd7e9eSX4xyU9394uWUe9utE4ffTzT6LY3Zlo/rJL8tyT/JcleC6rvjA38Pjonyddlmo75pUxTLy7u7u9aUsm7zjrvo71JbpLpcu2XJrlHpmm1r+/uhy6l4F1onT767ky/h96V5DOZrjz7y/O2b5n/88c2W+9n3dzmcUmekuTW3X3JsmrdrTbSRwttT4rF1HfcBj4zvDrTOlaPS3JJpp91d0ry9ZZI2VqCKrZNVd0z0y/DvUmuzDRs9Re6+2+r6qIkt1vjsB/3n+ydc7A+SvLzmS7JuhZ9tIMO0UcXJnnVvP2GSf4hyX/v7lcvqdRda52fdfdO8qRMo6iS5O+TPKm7z1tGrbvVOn100yTPSvJ9mUYkvi7Jo/0nbmcd4mfdKZlCxK/L9LPuw0lekunn3RXLqXZ3WqePfi5TQHV0pj76syRP9T7aWYf6WTfv/9skn+ju+y2vyt1tvT5aaHdSBFVLsc5nhhtnWk/sB5Ick+mPKL/Q3X+/rHoPV4IqAAAAAIZgjSoAAAAAhiCoAgAAAGAIgioAAAAAhiCoAgAAAGAIgioAAAAAhiCoAgAAAGAIgioAgC1SVT9WVV1VN152LQAAX40EVQAAAAAMQVAFAAAAwBAEVQAAm1RV96iqt1TVZVX12ao6r6q+7SBtn1ZVfz+3/WhVvbiqbrWqzYOq6vyq+lxV7a+qd1XVPRf2/2RVvb+qLq+qS6rqrVV153nfSfN0wx+oqufN9Xy0qn69qq63cI5TquqlVfWRqjpQVe+rqp9f1ea753Pdu6peM9dzYVV9b1UdUVW/NT//v1bVGWu81u+aaztQVZ+uqhdU1U224nsOAOwOgioAgE2oqu9O8uYkX0jyyCQPT/L2/7+9ewuVsgrjMP68aeUBEQujsKNRgdCFhJgFWkIXZhGakWVJRRAlgRfiForQ6EBCaajhAYqdIhRdVdrJEypW1IWURYYUkVihlKJeeMC3i/VNDOOMzMSOJnt+MOyZdWauNv9Za33AiBZdLgJeACYDs4GRwKZaQBQRVwPvAJuAO4EZwPvABVX9eGA5sBqYBDwC7ACGNsyzEDgCTAPWAM9U72tGALuBJ4DbgVXAAqCnyZpXANuBKcBP1fqWAkOA+6vPL0fE2Lrv5WZgA/BrNe/sap43WnwvkiRJp4nM/LfXIEmS9J8REZ8C5wJjsuEfqYh4iBLMDMnMI0369gMuBvYCEzJza0RMA1Zk5oUt5psD3JeZN7SovxL4EVidmTPryncC32Xm9CZ9AugHzAUezcyRVfktwGZgfmYuqMpGAd8AmzNzYlV2DrAP6M3MnqpsG3AyM2+tm2ciJdS7PjN3NVu/JElSPXdUSZIktSkiBgNjKQFNW7/2RcSkiNgREYeAk5SQCuDa6u/XwNCI6K2O2A1uGGInMDoiFlVHDs9rMdXHDZ+/BS6tW8eA6jjgHuAYZUfY88BVEdG/oe/Guvd7qr+bagWZeQr4gWoXWUQMAsYBb0dE/9qLsivrBNA0ZJMkSWpkUCVJktS+YUAAv7TTOCLGAO9SwqkHKWHOjVX1AIDM3A3cRTkSuB44EBFrI2J4Vb8BeBgYD2yp6pc1CbQONnw+Xpuj8hIwB1hJOZI3Bniufi3NxsrM422MP4yyQ+s1SjBVex2j7D67DEmSpDY0/nomSZKk1v4ATgGXtNl+CrAfuLe2AysirmhslJnrgHURMZRyl9ViYAkwvarvBXqr8GoqsAg4DMzrYO33AEsyc2GtICImd9D/TA4CCcynhG2N9vXRPJIk6SxnUCVJktSmzDwaEZ8DMyNiaRvH/wYCJxrazTjD+IeAtdUT/8Y1qd8PrIiIqcCoDpc/kLLDCfjrvqzT7q/6O6rv5TPgusx8ti/GlCRJ/08GVZIkSZ2ZR3m63QcRsRI4SgmVvmzS9hNgdkQsBt4DbgIeqG8QEY9V/T+k7Dy6hrL76c2qfgHlCYBbgAPAaGACne2mqq1lVnVH1e/ALOD8Dsc4k7nAxog4RXkq4GHgcsoOsacy8/s+nEuSJJ2lvKNKkiSpA5m5FbgNGASsAd6iBEd7m7RdD/QAd1PuqpoA3NHQ7CtgOPAK5UL0p4FVVT+ALyi7p5YDHwGPU47Yvdrh0p8EtgHLgNeBXcCLHY7RUmZup9yjNRxYTQnm5gI/A7/11TySJOnsFm0+sEaSJEmSJEn6R7mjSpIkSZIkSV3BoEqSJEmSJEldwaBKkiRJkiRJXcGgSpIkSZIkSV3BoEqSJEmSJEldwaBKkiRJkiRJXcGgSpIkSZIkSV3BoEqSJEmSJEldwaBKkiRJkiRJXeFPAc8B0kLnhwsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "nf = df['classname'].value_counts(sort=False)\n",
    "labels = df['classname'].value_counts(sort=False).index.tolist()\n",
    "y = np.array(nf)\n",
    "width = 1/1.5\n",
    "N = len(y)\n",
    "x = range(N)\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "ay = fig.add_subplot(211)\n",
    "\n",
    "plt.xticks(x, labels, size=15)\n",
    "plt.yticks(size=15)\n",
    "\n",
    "ay.bar(x, y, width, color=\"blue\")\n",
    "\n",
    "plt.title('Bar Chart',size=25)\n",
    "plt.xlabel('classname',size=15)\n",
    "plt.ylabel('Count',size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Create a CNN to Classify Driver Images (from Scratch)\n",
    "\n",
    "\n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "The images are rescaled by dividing every pixel in every image by 255. 0.5 is subtracted to ensure the mean is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 278.91it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 326.35it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 367.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255 - 0.5\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255 - 0.5\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255 - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "A CNN is created to classify driver images.  At the end of the code cell block, the layers of the model are summarized by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have created 4 convolutional layers with 4 max pooling layers in between. Filters were increased from 64 to 512 in each of the convolutional layers. Also dropout was used along with flattening layer before using the fully connected layer. Number of nodes in the last fully connected layer were setup as 10 along with softmax activation function. Relu activation function was used for all other layers.Xavier initialization was used in each of the layers.\n",
    "\n",
    "4 convolutional layers were used to learn hierarchy of high level features. Max pooling layer is added to reduce the dimensionality. Flatten layer is added to reduce the matrix to row vector. This is because fully connected layer only accepts row vector. Dropout layers were added to reduce overfitting and ensure that the network generalizes well. The last fully connected layer with softmax activation function is added to obtain probabilities of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/accubits/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 256)       131328    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 512)       524800    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               50176500  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 50,871,366\n",
      "Trainable params: 50,871,366\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,3), kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=256, kernel_size=2, padding='same', activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=512, kernel_size=2, padding='same', activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu', kernel_initializer='glorot_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax', kernel_initializer='glorot_normal'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "The model is trained in the code cell below. Model checkpointing is used to save the model that attains the best validation loss.\n",
    "\n",
    "Augmentation i.e. [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), can also be used as per the need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/30\n",
      "80/80 [==============================] - 13s 168ms/step - loss: 15.1965 - accuracy: 0.1250 - val_loss: 3.0012 - val_accuracy: 0.0500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.00116, saving model to weights.best.from_scratch.hdf5\n",
      "Epoch 2/30\n",
      "80/80 [==============================] - 12s 155ms/step - loss: 2.6886 - accuracy: 0.1250 - val_loss: 2.3147 - val_accuracy: 0.0500\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.00116 to 2.31468, saving model to weights.best.from_scratch.hdf5\n",
      "Epoch 3/30\n",
      "80/80 [==============================] - 18s 219ms/step - loss: 2.3142 - accuracy: 0.1000 - val_loss: 2.2969 - val_accuracy: 0.0500\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.31468 to 2.29685, saving model to weights.best.from_scratch.hdf5\n",
      "Epoch 4/30\n",
      "80/80 [==============================] - 13s 165ms/step - loss: 2.3529 - accuracy: 0.1125 - val_loss: 2.3340 - val_accuracy: 0.0500\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.29685\n",
      "Epoch 5/30\n",
      "80/80 [==============================] - 14s 169ms/step - loss: 2.2991 - accuracy: 0.1375 - val_loss: 2.3171 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.29685\n",
      "Epoch 6/30\n",
      "80/80 [==============================] - 15s 188ms/step - loss: 2.2896 - accuracy: 0.2250 - val_loss: 2.3335 - val_accuracy: 0.0500\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.29685\n",
      "Epoch 7/30\n",
      "80/80 [==============================] - 13s 159ms/step - loss: 2.2141 - accuracy: 0.2375 - val_loss: 2.3298 - val_accuracy: 0.0500\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.29685\n",
      "Epoch 8/30\n",
      "80/80 [==============================] - 15s 181ms/step - loss: 2.1396 - accuracy: 0.2750 - val_loss: 2.3381 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.29685\n",
      "Epoch 9/30\n",
      "80/80 [==============================] - 20s 247ms/step - loss: 2.2474 - accuracy: 0.2000 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.29685\n",
      "Epoch 10/30\n",
      "80/80 [==============================] - 15s 182ms/step - loss: 2.2313 - accuracy: 0.2750 - val_loss: 2.4274 - val_accuracy: 0.0500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.29685\n",
      "Epoch 11/30\n",
      "80/80 [==============================] - 14s 174ms/step - loss: 2.0959 - accuracy: 0.2750 - val_loss: 2.2578 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.29685 to 2.25777, saving model to weights.best.from_scratch.hdf5\n",
      "Epoch 12/30\n",
      "80/80 [==============================] - 15s 184ms/step - loss: 1.8330 - accuracy: 0.4000 - val_loss: 2.4856 - val_accuracy: 0.1500\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.25777\n",
      "Epoch 13/30\n",
      "80/80 [==============================] - 13s 165ms/step - loss: 1.8088 - accuracy: 0.3500 - val_loss: 2.2902 - val_accuracy: 0.1500\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.25777\n",
      "Epoch 14/30\n",
      "80/80 [==============================] - 12s 151ms/step - loss: 1.6738 - accuracy: 0.4000 - val_loss: 2.2775 - val_accuracy: 0.2000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.25777\n",
      "Epoch 15/30\n",
      "80/80 [==============================] - 13s 157ms/step - loss: 1.5543 - accuracy: 0.5375 - val_loss: 2.4312 - val_accuracy: 0.2000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.25777\n",
      "Epoch 16/30\n",
      "80/80 [==============================] - 13s 161ms/step - loss: 1.3430 - accuracy: 0.5875 - val_loss: 2.1558 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.25777 to 2.15580, saving model to weights.best.from_scratch.hdf5\n",
      "Epoch 17/30\n",
      "80/80 [==============================] - 15s 191ms/step - loss: 1.2858 - accuracy: 0.5750 - val_loss: 2.6730 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.15580\n",
      "Epoch 18/30\n",
      "80/80 [==============================] - 19s 239ms/step - loss: 1.3588 - accuracy: 0.5375 - val_loss: 2.5198 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.15580\n",
      "Epoch 19/30\n",
      "80/80 [==============================] - 12s 153ms/step - loss: 1.0673 - accuracy: 0.6375 - val_loss: 2.0268 - val_accuracy: 0.3000\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.15580 to 2.02676, saving model to weights.best.from_scratch.hdf5\n",
      "Epoch 20/30\n",
      "80/80 [==============================] - 12s 153ms/step - loss: 0.8398 - accuracy: 0.7125 - val_loss: 2.1693 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.02676\n",
      "Epoch 21/30\n",
      "80/80 [==============================] - 12s 149ms/step - loss: 0.7734 - accuracy: 0.7875 - val_loss: 2.2084 - val_accuracy: 0.3000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.02676\n",
      "Epoch 22/30\n",
      "80/80 [==============================] - 12s 150ms/step - loss: 0.8745 - accuracy: 0.7000 - val_loss: 2.2245 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.02676\n",
      "Epoch 23/30\n",
      "80/80 [==============================] - 13s 157ms/step - loss: 0.2916 - accuracy: 0.9250 - val_loss: 2.5561 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.02676\n",
      "Epoch 24/30\n",
      "80/80 [==============================] - 15s 184ms/step - loss: 0.8386 - accuracy: 0.7125 - val_loss: 2.4484 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.02676\n",
      "Epoch 25/30\n",
      "80/80 [==============================] - 13s 165ms/step - loss: 0.5824 - accuracy: 0.8125 - val_loss: 2.4544 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.02676\n",
      "Epoch 26/30\n",
      "80/80 [==============================] - 13s 164ms/step - loss: 0.4187 - accuracy: 0.8750 - val_loss: 2.1927 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 2.02676\n",
      "Epoch 27/30\n",
      "80/80 [==============================] - 13s 163ms/step - loss: 0.1919 - accuracy: 0.9625 - val_loss: 2.8137 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.02676\n",
      "Epoch 28/30\n",
      "80/80 [==============================] - 13s 165ms/step - loss: 0.1710 - accuracy: 0.9625 - val_loss: 2.7297 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.02676\n",
      "Epoch 29/30\n",
      "80/80 [==============================] - 14s 176ms/step - loss: 0.0556 - accuracy: 0.9875 - val_loss: 3.5138 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.02676\n",
      "Epoch 30/30\n",
      "80/80 [==============================] - 16s 206ms/step - loss: 0.1685 - accuracy: 0.9250 - val_loss: 2.6131 - val_accuracy: 0.3500\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.02676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f2ce218e208>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "epochs = 30 # epochs used is 30 and batch size is 40\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=40, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_final = [item_test[15:] for item_test in test_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "The model is tried on the test dataset of driver images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in test_tensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = np.column_stack((np.asarray(test_files_final), np.asarray(predictions,dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['t/img_20357.jpg' '0.027232496' '0.083712384' '0.33201015' '0.09262748'\n",
      "  '0.026723994' '0.016407846' '0.31632018' '0.0068669035' '0.033907004'\n",
      "  '0.06419161']\n",
      " ['t/img_20356.jpg' '0.09524828' '0.15722592' '0.1372237' '0.18128169'\n",
      "  '0.06398636' '0.012606534' '0.08220812' '0.05985189' '0.15875508'\n",
      "  '0.0516124']\n",
      " ['t/img_47488.jpg' '0.0013499353' '0.0021703807' '0.008874771'\n",
      "  '0.047901012' '0.03978037' '0.8900887' '0.0015866857' '0.005180446'\n",
      "  '0.00035804746' '0.00270958']]\n"
     ]
    }
   ],
   "source": [
    "print(subm[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['t/img_20357.jpg' '0.027232496' '0.083712384' '0.33201015' '0.09262748'\n",
      "  '0.026723994' '0.016407846' '0.31632018' '0.0068669035' '0.033907004'\n",
      "  '0.06419161']\n",
      " ['t/img_20356.jpg' '0.09524828' '0.15722592' '0.1372237' '0.18128169'\n",
      "  '0.06398636' '0.012606534' '0.08220812' '0.05985189' '0.15875508'\n",
      "  '0.0516124']\n",
      " ['t/img_47488.jpg' '0.0013499353' '0.0021703807' '0.008874771'\n",
      "  '0.047901012' '0.03978037' '0.8900887' '0.0015866857' '0.005180446'\n",
      "  '0.00035804746' '0.00270958']\n",
      " ['t/img_12.jpg' '0.05929871' '0.069039166' '0.023994142' '0.037922535'\n",
      "  '0.13494915' '0.048393004' '0.1498171' '0.4398979' '0.029175928'\n",
      "  '0.0075123594']\n",
      " ['t/img_35294.jpg' '0.0026175962' '0.12725244' '0.1665128' '0.030762391'\n",
      "  '0.009590377' '0.008203086' '0.08893084' '0.009380303' '0.43914044'\n",
      "  '0.1176097']]\n"
     ]
    }
   ],
   "source": [
    "print(subm[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='kaggle_submissions/submission.csv' target='_blank'>kaggle_submissions/submission.csv</a><br>"
      ],
      "text/plain": [
       "/code/home/aind2/capstone/kaggle_submissions/submission.csv"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('kaggle_submissions/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission resulted in Public Score of 2.67118. This can result in rank of 1362 out of 1440 in Public Leaderboard i.e. in top 94.58%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Train a CNN with Transfer Learning - Part1\n",
    "\n",
    "To reduce training time without sacrificing accuracy, a CNN is trained using transfer learning.  \n",
    "\n",
    "### Using the bottleneck features of a pre-trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 43s 1us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16(include_top=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features_train_VGG16 = np.asarray([model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in train_tensors],dtype=np.float32)\n",
    "#bottleneck_features_train_VGG16 = model.predict_generator(generator, nb_train_samples // batch_size)\n",
    "np.save(open('bottleneck_features/bottleneck_features_train_VGG16.npy', 'wb'),bottleneck_features_train_VGG16)\n",
    "\n",
    "\n",
    "bottleneck_features_valid_VGG16 = np.asarray([model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in valid_tensors],dtype=np.float32)\n",
    "#bottleneck_features_train_VGG16 = model.predict_generator(generator, nb_train_samples // batch_size)\n",
    "np.save(open('bottleneck_features/bottleneck_features_valid_VGG16.npy', 'wb'),bottleneck_features_valid_VGG16)\n",
    "\n",
    "\n",
    "bottleneck_features_test_VGG16 = np.asarray([model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in test_tensors],dtype=np.float32)\n",
    "np.save(open('bottleneck_features/bottleneck_features_test_VGG16.npy', 'wb'),bottleneck_features_test_VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17939, 7, 7, 512)\n",
      "(4485, 7, 7, 512)\n",
      "(79726, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "print(bottleneck_features_train_VGG16.shape)\n",
    "print(bottleneck_features_valid_VGG16.shape)\n",
    "print(bottleneck_features_test_VGG16.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture1\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each driver category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 5,130\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=bottleneck_features_train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(10, activation='softmax', kernel_initializer='glorot_normal'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17939 samples, validate on 4485 samples\n",
      "Epoch 1/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 2.0647 - acc: 0.3629Epoch 00000: val_loss improved from inf to 1.83891, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 2.0640 - acc: 0.3635 - val_loss: 1.8389 - val_acc: 0.4916\n",
      "Epoch 2/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 1.6720 - acc: 0.5993Epoch 00001: val_loss improved from 1.83891 to 1.52233, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 1.6718 - acc: 0.5994 - val_loss: 1.5223 - val_acc: 0.6553\n",
      "Epoch 3/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 1.4062 - acc: 0.7006Epoch 00002: val_loss improved from 1.52233 to 1.30828, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 1.4050 - acc: 0.7011 - val_loss: 1.3083 - val_acc: 0.7041\n",
      "Epoch 4/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 1.2111 - acc: 0.7550Epoch 00003: val_loss improved from 1.30828 to 1.13829, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 1.2105 - acc: 0.7553 - val_loss: 1.1383 - val_acc: 0.7670\n",
      "Epoch 5/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 1.0652 - acc: 0.7947Epoch 00004: val_loss improved from 1.13829 to 1.00286, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 1.0649 - acc: 0.7950 - val_loss: 1.0029 - val_acc: 0.8107\n",
      "Epoch 6/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.9489 - acc: 0.8183Epoch 00005: val_loss improved from 1.00286 to 0.89713, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.9485 - acc: 0.8183 - val_loss: 0.8971 - val_acc: 0.8348\n",
      "Epoch 7/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.8545 - acc: 0.8403Epoch 00006: val_loss improved from 0.89713 to 0.81219, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.8543 - acc: 0.8403 - val_loss: 0.8122 - val_acc: 0.8508\n",
      "Epoch 8/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.7787 - acc: 0.8533Epoch 00007: val_loss improved from 0.81219 to 0.74231, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.7785 - acc: 0.8534 - val_loss: 0.7423 - val_acc: 0.8627\n",
      "Epoch 9/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.7135 - acc: 0.8668Epoch 00008: val_loss improved from 0.74231 to 0.69014, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.7131 - acc: 0.8670 - val_loss: 0.6901 - val_acc: 0.8682\n",
      "Epoch 10/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.6585 - acc: 0.8790Epoch 00009: val_loss improved from 0.69014 to 0.63645, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.6587 - acc: 0.8789 - val_loss: 0.6365 - val_acc: 0.8905\n",
      "Epoch 11/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.6130 - acc: 0.8860Epoch 00010: val_loss improved from 0.63645 to 0.59603, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.6122 - acc: 0.8864 - val_loss: 0.5960 - val_acc: 0.8903\n",
      "Epoch 12/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.5717 - acc: 0.8937Epoch 00011: val_loss improved from 0.59603 to 0.55391, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.5717 - acc: 0.8938 - val_loss: 0.5539 - val_acc: 0.8932\n",
      "Epoch 13/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.5353 - acc: 0.9026Epoch 00012: val_loss improved from 0.55391 to 0.51687, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.5353 - acc: 0.9026 - val_loss: 0.5169 - val_acc: 0.9057\n",
      "Epoch 14/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.5046 - acc: 0.9057Epoch 00013: val_loss improved from 0.51687 to 0.48481, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.5043 - acc: 0.9058 - val_loss: 0.4848 - val_acc: 0.9097\n",
      "Epoch 15/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.4774 - acc: 0.9115Epoch 00014: val_loss improved from 0.48481 to 0.46939, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.4770 - acc: 0.9117 - val_loss: 0.4694 - val_acc: 0.9084\n",
      "Epoch 16/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.4519 - acc: 0.9166Epoch 00015: val_loss improved from 0.46939 to 0.44187, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.4520 - acc: 0.9164 - val_loss: 0.4419 - val_acc: 0.9177\n",
      "Epoch 17/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.4305 - acc: 0.9197Epoch 00016: val_loss improved from 0.44187 to 0.42105, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.4302 - acc: 0.9198 - val_loss: 0.4211 - val_acc: 0.9186\n",
      "Epoch 18/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.9246Epoch 00017: val_loss improved from 0.42105 to 0.40389, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.4095 - acc: 0.9245 - val_loss: 0.4039 - val_acc: 0.9260\n",
      "Epoch 19/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.3915 - acc: 0.9276Epoch 00018: val_loss improved from 0.40389 to 0.38952, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.3915 - acc: 0.9275 - val_loss: 0.3895 - val_acc: 0.9197\n",
      "Epoch 20/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.3756 - acc: 0.9296Epoch 00019: val_loss improved from 0.38952 to 0.37208, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.3751 - acc: 0.9298 - val_loss: 0.3721 - val_acc: 0.9293\n",
      "Epoch 21/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.9321Epoch 00020: val_loss improved from 0.37208 to 0.35270, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.3607 - acc: 0.9319 - val_loss: 0.3527 - val_acc: 0.9329\n",
      "Epoch 22/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.3467 - acc: 0.9344Epoch 00021: val_loss improved from 0.35270 to 0.34276, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.3465 - acc: 0.9344 - val_loss: 0.3428 - val_acc: 0.9338\n",
      "Epoch 23/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.9366Epoch 00022: val_loss improved from 0.34276 to 0.33295, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.3328 - acc: 0.9367 - val_loss: 0.3330 - val_acc: 0.9409\n",
      "Epoch 24/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.9382Epoch 00023: val_loss improved from 0.33295 to 0.31651, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.3219 - acc: 0.9380 - val_loss: 0.3165 - val_acc: 0.9396\n",
      "Epoch 25/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.3110 - acc: 0.9404Epoch 00024: val_loss improved from 0.31651 to 0.30703, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.3109 - acc: 0.9405 - val_loss: 0.3070 - val_acc: 0.9387\n",
      "Epoch 26/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.3012 - acc: 0.9422Epoch 00025: val_loss improved from 0.30703 to 0.29385, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.3007 - acc: 0.9423 - val_loss: 0.2938 - val_acc: 0.9425\n",
      "Epoch 27/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.2910 - acc: 0.9436Epoch 00026: val_loss improved from 0.29385 to 0.28642, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.2912 - acc: 0.9435 - val_loss: 0.2864 - val_acc: 0.9460\n",
      "Epoch 28/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.2827 - acc: 0.9440Epoch 00027: val_loss improved from 0.28642 to 0.27781, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.2826 - acc: 0.9439 - val_loss: 0.2778 - val_acc: 0.9454\n",
      "Epoch 29/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.2742 - acc: 0.9468Epoch 00028: val_loss improved from 0.27781 to 0.27216, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.2742 - acc: 0.9469 - val_loss: 0.2722 - val_acc: 0.9469\n",
      "Epoch 30/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.2667 - acc: 0.9465Epoch 00029: val_loss improved from 0.27216 to 0.25916, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.2668 - acc: 0.9465 - val_loss: 0.2592 - val_acc: 0.9501\n",
      "Epoch 31/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.2593 - acc: 0.9487Epoch 00030: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.2592 - acc: 0.9487 - val_loss: 0.2598 - val_acc: 0.9480\n",
      "Epoch 32/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.2525 - acc: 0.9504Epoch 00031: val_loss improved from 0.25916 to 0.24771, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.2526 - acc: 0.9504 - val_loss: 0.2477 - val_acc: 0.9496\n",
      "Epoch 33/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.2462 - acc: 0.9516Epoch 00032: val_loss improved from 0.24771 to 0.24715, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.2462 - acc: 0.9516 - val_loss: 0.2471 - val_acc: 0.9465\n",
      "Epoch 34/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.2408 - acc: 0.9526Epoch 00033: val_loss improved from 0.24715 to 0.23568, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.2407 - acc: 0.9527 - val_loss: 0.2357 - val_acc: 0.9507\n",
      "Epoch 35/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.2343 - acc: 0.9537Epoch 00034: val_loss improved from 0.23568 to 0.23409, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.2343 - acc: 0.9537 - val_loss: 0.2341 - val_acc: 0.9534\n",
      "Epoch 36/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.2293 - acc: 0.9544Epoch 00035: val_loss improved from 0.23409 to 0.22492, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.2294 - acc: 0.9543 - val_loss: 0.2249 - val_acc: 0.9559\n",
      "Epoch 37/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9551Epoch 00036: val_loss improved from 0.22492 to 0.22115, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.2243 - acc: 0.9550 - val_loss: 0.2212 - val_acc: 0.9574\n",
      "Epoch 38/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9560Epoch 00037: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.2198 - acc: 0.9560 - val_loss: 0.2221 - val_acc: 0.9530\n",
      "Epoch 39/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.2148 - acc: 0.9560Epoch 00038: val_loss improved from 0.22115 to 0.21467, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.2149 - acc: 0.9558 - val_loss: 0.2147 - val_acc: 0.9545\n",
      "Epoch 40/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.2107 - acc: 0.9575Epoch 00039: val_loss improved from 0.21467 to 0.20833, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.2106 - acc: 0.9575 - val_loss: 0.2083 - val_acc: 0.9545\n",
      "Epoch 41/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.2068 - acc: 0.9578Epoch 00040: val_loss improved from 0.20833 to 0.20481, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.2066 - acc: 0.9579 - val_loss: 0.2048 - val_acc: 0.9579\n",
      "Epoch 42/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9586Epoch 00041: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.2027 - acc: 0.9587 - val_loss: 0.2077 - val_acc: 0.9550\n",
      "Epoch 43/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9599Epoch 00042: val_loss improved from 0.20481 to 0.20144, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1993 - acc: 0.9598 - val_loss: 0.2014 - val_acc: 0.9570\n",
      "Epoch 44/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.9603Epoch 00043: val_loss improved from 0.20144 to 0.19747, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1951 - acc: 0.9603 - val_loss: 0.1975 - val_acc: 0.9583\n",
      "Epoch 45/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1920 - acc: 0.9609Epoch 00044: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1921 - acc: 0.9609 - val_loss: 0.2033 - val_acc: 0.9603\n",
      "Epoch 46/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9608Epoch 00045: val_loss improved from 0.19747 to 0.18833, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1887 - acc: 0.9609 - val_loss: 0.1883 - val_acc: 0.9588\n",
      "Epoch 47/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1855 - acc: 0.9620Epoch 00046: val_loss improved from 0.18833 to 0.18571, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1855 - acc: 0.9620 - val_loss: 0.1857 - val_acc: 0.9605\n",
      "Epoch 48/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.9625Epoch 00047: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1822 - acc: 0.9624 - val_loss: 0.1870 - val_acc: 0.9588\n",
      "Epoch 49/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1792 - acc: 0.9643Epoch 00048: val_loss improved from 0.18571 to 0.18181, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1793 - acc: 0.9643 - val_loss: 0.1818 - val_acc: 0.9621\n",
      "Epoch 50/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9632Epoch 00049: val_loss improved from 0.18181 to 0.17893, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1767 - acc: 0.9632 - val_loss: 0.1789 - val_acc: 0.9630\n",
      "Epoch 51/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.9641Epoch 00050: val_loss improved from 0.17893 to 0.17376, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1741 - acc: 0.9641 - val_loss: 0.1738 - val_acc: 0.9639\n",
      "Epoch 52/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1712 - acc: 0.9643Epoch 00051: val_loss improved from 0.17376 to 0.17256, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1713 - acc: 0.9643 - val_loss: 0.1726 - val_acc: 0.9652\n",
      "Epoch 53/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1692 - acc: 0.9645Epoch 00052: val_loss improved from 0.17256 to 0.17007, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1691 - acc: 0.9645 - val_loss: 0.1701 - val_acc: 0.9637\n",
      "Epoch 54/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.1659 - acc: 0.9653Epoch 00053: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1660 - acc: 0.9653 - val_loss: 0.1708 - val_acc: 0.9632\n",
      "Epoch 55/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1638 - acc: 0.9658Epoch 00054: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1639 - acc: 0.9657 - val_loss: 0.1732 - val_acc: 0.9628\n",
      "Epoch 56/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.9656Epoch 00055: val_loss improved from 0.17007 to 0.16529, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1619 - acc: 0.9655 - val_loss: 0.1653 - val_acc: 0.9634\n",
      "Epoch 57/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9661Epoch 00056: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1596 - acc: 0.9663 - val_loss: 0.1662 - val_acc: 0.9621\n",
      "Epoch 58/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1578 - acc: 0.9659Epoch 00057: val_loss improved from 0.16529 to 0.16305, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1577 - acc: 0.9659 - val_loss: 0.1630 - val_acc: 0.9634\n",
      "Epoch 59/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1554 - acc: 0.9672Epoch 00058: val_loss improved from 0.16305 to 0.16189, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1556 - acc: 0.9672 - val_loss: 0.1619 - val_acc: 0.9634\n",
      "Epoch 60/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1537 - acc: 0.9672Epoch 00059: val_loss improved from 0.16189 to 0.15839, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1537 - acc: 0.9672 - val_loss: 0.1584 - val_acc: 0.9659\n",
      "Epoch 61/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.9680Epoch 00060: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1514 - acc: 0.9682 - val_loss: 0.1600 - val_acc: 0.9632\n",
      "Epoch 62/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.9682Epoch 00061: val_loss improved from 0.15839 to 0.15189, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1496 - acc: 0.9683 - val_loss: 0.1519 - val_acc: 0.9661\n",
      "Epoch 63/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9686Epoch 00062: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1479 - acc: 0.9686 - val_loss: 0.1556 - val_acc: 0.9670\n",
      "Epoch 64/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1458 - acc: 0.9692Epoch 00063: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1456 - acc: 0.9693 - val_loss: 0.1547 - val_acc: 0.9654\n",
      "Epoch 65/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.9698Epoch 00064: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1442 - acc: 0.9698 - val_loss: 0.1535 - val_acc: 0.9639\n",
      "Epoch 66/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9698Epoch 00065: val_loss improved from 0.15189 to 0.14921, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1429 - acc: 0.9698 - val_loss: 0.1492 - val_acc: 0.9679\n",
      "Epoch 67/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9700Epoch 00066: val_loss improved from 0.14921 to 0.14502, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1414 - acc: 0.9702 - val_loss: 0.1450 - val_acc: 0.9690\n",
      "Epoch 68/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9697Epoch 00067: val_loss improved from 0.14502 to 0.14244, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1395 - acc: 0.9697 - val_loss: 0.1424 - val_acc: 0.9679\n",
      "Epoch 69/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.9708Epoch 00068: val_loss improved from 0.14244 to 0.14215, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1377 - acc: 0.9708 - val_loss: 0.1422 - val_acc: 0.9686\n",
      "Epoch 70/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.9715Epoch 00069: val_loss improved from 0.14215 to 0.13981, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1368 - acc: 0.9715 - val_loss: 0.1398 - val_acc: 0.9699\n",
      "Epoch 71/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.9719Epoch 00070: val_loss improved from 0.13981 to 0.13892, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1352 - acc: 0.9719 - val_loss: 0.1389 - val_acc: 0.9697\n",
      "Epoch 72/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9711Epoch 00071: val_loss improved from 0.13892 to 0.13802, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1346 - acc: 0.9708 - val_loss: 0.1380 - val_acc: 0.9712\n",
      "Epoch 73/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.1325 - acc: 0.9716Epoch 00072: val_loss improved from 0.13802 to 0.13740, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1329 - acc: 0.9716 - val_loss: 0.1374 - val_acc: 0.9688\n",
      "Epoch 74/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1311 - acc: 0.9726Epoch 00073: val_loss improved from 0.13740 to 0.13657, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1311 - acc: 0.9726 - val_loss: 0.1366 - val_acc: 0.9686\n",
      "Epoch 75/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1302 - acc: 0.9712Epoch 00074: val_loss improved from 0.13657 to 0.13542, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1300 - acc: 0.9712 - val_loss: 0.1354 - val_acc: 0.9666\n",
      "Epoch 76/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1285 - acc: 0.9718Epoch 00075: val_loss improved from 0.13542 to 0.13231, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1287 - acc: 0.9717 - val_loss: 0.1323 - val_acc: 0.9706\n",
      "Epoch 77/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.1274 - acc: 0.9732Epoch 00076: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1272 - acc: 0.9732 - val_loss: 0.1339 - val_acc: 0.9677\n",
      "Epoch 78/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.1263 - acc: 0.9733Epoch 00077: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1264 - acc: 0.9731 - val_loss: 0.1357 - val_acc: 0.9690\n",
      "Epoch 79/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1252 - acc: 0.9736Epoch 00078: val_loss improved from 0.13231 to 0.13018, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1252 - acc: 0.9736 - val_loss: 0.1302 - val_acc: 0.9710\n",
      "Epoch 80/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.1243 - acc: 0.9735Epoch 00079: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1242 - acc: 0.9734 - val_loss: 0.1308 - val_acc: 0.9692\n",
      "Epoch 81/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1226 - acc: 0.9733Epoch 00080: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1225 - acc: 0.9733 - val_loss: 0.1332 - val_acc: 0.9679\n",
      "Epoch 82/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1221 - acc: 0.9739Epoch 00081: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1222 - acc: 0.9739 - val_loss: 0.1317 - val_acc: 0.9686\n",
      "Epoch 83/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1209 - acc: 0.9739Epoch 00082: val_loss improved from 0.13018 to 0.12571, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1208 - acc: 0.9740 - val_loss: 0.1257 - val_acc: 0.9715\n",
      "Epoch 84/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.1201 - acc: 0.9737Epoch 00083: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1200 - acc: 0.9738 - val_loss: 0.1268 - val_acc: 0.9708\n",
      "Epoch 85/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1184 - acc: 0.9754Epoch 00084: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1184 - acc: 0.9754 - val_loss: 0.1310 - val_acc: 0.9681\n",
      "Epoch 86/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1180 - acc: 0.9743Epoch 00085: val_loss improved from 0.12571 to 0.12398, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1178 - acc: 0.9744 - val_loss: 0.1240 - val_acc: 0.9706\n",
      "Epoch 87/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.1167 - acc: 0.9751Epoch 00086: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1169 - acc: 0.9750 - val_loss: 0.1247 - val_acc: 0.9701\n",
      "Epoch 88/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9754Epoch 00087: val_loss improved from 0.12398 to 0.11966, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1156 - acc: 0.9754 - val_loss: 0.1197 - val_acc: 0.9724\n",
      "Epoch 89/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1151 - acc: 0.9756Epoch 00088: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1150 - acc: 0.9757 - val_loss: 0.1214 - val_acc: 0.9721\n",
      "Epoch 90/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.1132 - acc: 0.9756Epoch 00089: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1135 - acc: 0.9756 - val_loss: 0.1198 - val_acc: 0.9712\n",
      "Epoch 91/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.1128 - acc: 0.9754Epoch 00090: val_loss improved from 0.11966 to 0.11788, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1130 - acc: 0.9754 - val_loss: 0.1179 - val_acc: 0.9706\n",
      "Epoch 92/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.1120 - acc: 0.9758Epoch 00091: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1123 - acc: 0.9759 - val_loss: 0.1180 - val_acc: 0.9719\n",
      "Epoch 93/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.1116 - acc: 0.9763Epoch 00092: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1114 - acc: 0.9763 - val_loss: 0.1180 - val_acc: 0.9726\n",
      "Epoch 94/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1106 - acc: 0.9760Epoch 00093: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1105 - acc: 0.9760 - val_loss: 0.1201 - val_acc: 0.9721\n",
      "Epoch 95/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9764Epoch 00094: val_loss improved from 0.11788 to 0.11686, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1097 - acc: 0.9763 - val_loss: 0.1169 - val_acc: 0.9710\n",
      "Epoch 96/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.1088 - acc: 0.9762Epoch 00095: val_loss improved from 0.11686 to 0.11596, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1087 - acc: 0.9762 - val_loss: 0.1160 - val_acc: 0.9706\n",
      "Epoch 97/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.1083 - acc: 0.9767Epoch 00096: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1080 - acc: 0.9768 - val_loss: 0.1173 - val_acc: 0.9710\n",
      "Epoch 98/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.1074 - acc: 0.9776Epoch 00097: val_loss improved from 0.11596 to 0.11469, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1073 - acc: 0.9776 - val_loss: 0.1147 - val_acc: 0.9735\n",
      "Epoch 99/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9773Epoch 00098: val_loss improved from 0.11469 to 0.11293, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1063 - acc: 0.9774 - val_loss: 0.1129 - val_acc: 0.9715\n",
      "Epoch 100/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.1060 - acc: 0.9780Epoch 00099: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1058 - acc: 0.9781 - val_loss: 0.1137 - val_acc: 0.9719\n",
      "Epoch 101/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9778Epoch 00100: val_loss improved from 0.11293 to 0.11097, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1050 - acc: 0.9776 - val_loss: 0.1110 - val_acc: 0.9719\n",
      "Epoch 102/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.1045 - acc: 0.9772Epoch 00101: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1042 - acc: 0.9773 - val_loss: 0.1116 - val_acc: 0.9737\n",
      "Epoch 103/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.1036 - acc: 0.9770Epoch 00102: val_loss improved from 0.11097 to 0.10998, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1037 - acc: 0.9770 - val_loss: 0.1100 - val_acc: 0.9728\n",
      "Epoch 104/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9780Epoch 00103: val_loss improved from 0.10998 to 0.10911, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.1030 - acc: 0.9780 - val_loss: 0.1091 - val_acc: 0.9741\n",
      "Epoch 105/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.1018 - acc: 0.9783Epoch 00104: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.1019 - acc: 0.9782 - val_loss: 0.1118 - val_acc: 0.9730\n",
      "Epoch 106/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.1016 - acc: 0.9771Epoch 00105: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1017 - acc: 0.9771 - val_loss: 0.1114 - val_acc: 0.9724\n",
      "Epoch 107/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.1009 - acc: 0.9779Epoch 00106: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1009 - acc: 0.9779 - val_loss: 0.1095 - val_acc: 0.9719\n",
      "Epoch 108/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.1007 - acc: 0.9783Epoch 00107: val_loss improved from 0.10911 to 0.10691, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.1008 - acc: 0.9783 - val_loss: 0.1069 - val_acc: 0.9721\n",
      "Epoch 109/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0996 - acc: 0.9781Epoch 00108: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.1000 - acc: 0.9780 - val_loss: 0.1121 - val_acc: 0.9710\n",
      "Epoch 110/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0990 - acc: 0.9784Epoch 00109: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0991 - acc: 0.9784 - val_loss: 0.1079 - val_acc: 0.9728\n",
      "Epoch 111/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9787Epoch 00110: val_loss improved from 0.10691 to 0.10639, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0985 - acc: 0.9788 - val_loss: 0.1064 - val_acc: 0.9741\n",
      "Epoch 112/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0981 - acc: 0.9788Epoch 00111: val_loss improved from 0.10639 to 0.10635, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0979 - acc: 0.9789 - val_loss: 0.1063 - val_acc: 0.9735\n",
      "Epoch 113/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9788Epoch 00112: val_loss improved from 0.10635 to 0.10510, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0975 - acc: 0.9788 - val_loss: 0.1051 - val_acc: 0.9737\n",
      "Epoch 114/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9788Epoch 00113: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0964 - acc: 0.9789 - val_loss: 0.1060 - val_acc: 0.9744\n",
      "Epoch 115/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0962 - acc: 0.9793Epoch 00114: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0961 - acc: 0.9793 - val_loss: 0.1054 - val_acc: 0.9739\n",
      "Epoch 116/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0955 - acc: 0.9790Epoch 00115: val_loss improved from 0.10510 to 0.10446, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0955 - acc: 0.9790 - val_loss: 0.1045 - val_acc: 0.9728\n",
      "Epoch 117/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0951 - acc: 0.9794Epoch 00116: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0952 - acc: 0.9793 - val_loss: 0.1051 - val_acc: 0.9735\n",
      "Epoch 118/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9793Epoch 00117: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0945 - acc: 0.9794 - val_loss: 0.1049 - val_acc: 0.9717\n",
      "Epoch 119/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9803Epoch 00118: val_loss improved from 0.10446 to 0.10425, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0942 - acc: 0.9803 - val_loss: 0.1042 - val_acc: 0.9724\n",
      "Epoch 120/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9795Epoch 00119: val_loss improved from 0.10425 to 0.10324, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0934 - acc: 0.9795 - val_loss: 0.1032 - val_acc: 0.9735\n",
      "Epoch 121/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9800Epoch 00120: val_loss improved from 0.10324 to 0.10121, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0931 - acc: 0.9802 - val_loss: 0.1012 - val_acc: 0.9744\n",
      "Epoch 122/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9798Epoch 00121: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0925 - acc: 0.9798 - val_loss: 0.1019 - val_acc: 0.9737\n",
      "Epoch 123/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0917 - acc: 0.9799Epoch 00122: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0916 - acc: 0.9799 - val_loss: 0.1039 - val_acc: 0.9739\n",
      "Epoch 124/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0916 - acc: 0.9803Epoch 00123: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0914 - acc: 0.9804 - val_loss: 0.1014 - val_acc: 0.9737\n",
      "Epoch 125/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9797Epoch 00124: val_loss improved from 0.10121 to 0.09904, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0908 - acc: 0.9797 - val_loss: 0.0990 - val_acc: 0.9748\n",
      "Epoch 126/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9799Epoch 00125: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0904 - acc: 0.9799 - val_loss: 0.0998 - val_acc: 0.9744\n",
      "Epoch 127/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9801Epoch 00126: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0901 - acc: 0.9802 - val_loss: 0.1010 - val_acc: 0.9746\n",
      "Epoch 128/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0896 - acc: 0.9797Epoch 00127: val_loss improved from 0.09904 to 0.09765, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0896 - acc: 0.9798 - val_loss: 0.0976 - val_acc: 0.9748\n",
      "Epoch 129/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0889 - acc: 0.9806Epoch 00128: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0892 - acc: 0.9807 - val_loss: 0.0979 - val_acc: 0.9735\n",
      "Epoch 130/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0887 - acc: 0.9805Epoch 00129: val_loss improved from 0.09765 to 0.09754, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0886 - acc: 0.9805 - val_loss: 0.0975 - val_acc: 0.9759\n",
      "Epoch 131/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9801Epoch 00130: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0883 - acc: 0.9802 - val_loss: 0.0990 - val_acc: 0.9753\n",
      "Epoch 132/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9809Epoch 00131: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0875 - acc: 0.9809 - val_loss: 0.1000 - val_acc: 0.9737\n",
      "Epoch 133/400\n",
      "17776/17939 [============================>.] - ETA: 0s - loss: 0.0872 - acc: 0.9808Epoch 00132: val_loss improved from 0.09754 to 0.09641, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0874 - acc: 0.9807 - val_loss: 0.0964 - val_acc: 0.9753\n",
      "Epoch 134/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9806Epoch 00133: val_loss improved from 0.09641 to 0.09622, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0872 - acc: 0.9804 - val_loss: 0.0962 - val_acc: 0.9744\n",
      "Epoch 135/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9809Epoch 00134: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0862 - acc: 0.9810 - val_loss: 0.0994 - val_acc: 0.9717\n",
      "Epoch 136/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9813Epoch 00135: val_loss improved from 0.09622 to 0.09595, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0859 - acc: 0.9813 - val_loss: 0.0959 - val_acc: 0.9750\n",
      "Epoch 137/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0856 - acc: 0.9808Epoch 00136: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0857 - acc: 0.9808 - val_loss: 0.0962 - val_acc: 0.9746\n",
      "Epoch 138/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0853 - acc: 0.9811Epoch 00137: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0853 - acc: 0.9810 - val_loss: 0.0969 - val_acc: 0.9744\n",
      "Epoch 139/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9809Epoch 00138: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0845 - acc: 0.9810 - val_loss: 0.0966 - val_acc: 0.9735\n",
      "Epoch 140/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9811Epoch 00139: val_loss improved from 0.09595 to 0.09464, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0844 - acc: 0.9811 - val_loss: 0.0946 - val_acc: 0.9755\n",
      "Epoch 141/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0842 - acc: 0.9818Epoch 00140: val_loss improved from 0.09464 to 0.09430, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0842 - acc: 0.9818 - val_loss: 0.0943 - val_acc: 0.9741\n",
      "Epoch 142/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9818Epoch 00141: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0833 - acc: 0.9818 - val_loss: 0.0951 - val_acc: 0.9739\n",
      "Epoch 143/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9817Epoch 00142: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0834 - acc: 0.9817 - val_loss: 0.0945 - val_acc: 0.9748\n",
      "Epoch 144/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9815Epoch 00143: val_loss improved from 0.09430 to 0.09382, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0829 - acc: 0.9815 - val_loss: 0.0938 - val_acc: 0.9755\n",
      "Epoch 145/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0826 - acc: 0.9808Epoch 00144: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0825 - acc: 0.9809 - val_loss: 0.0947 - val_acc: 0.9750\n",
      "Epoch 146/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0814 - acc: 0.9814Epoch 00145: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0819 - acc: 0.9814 - val_loss: 0.0963 - val_acc: 0.9735\n",
      "Epoch 147/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0819 - acc: 0.9821Epoch 00146: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0817 - acc: 0.9822 - val_loss: 0.0946 - val_acc: 0.9750\n",
      "Epoch 148/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0809 - acc: 0.9812Epoch 00147: val_loss improved from 0.09382 to 0.09172, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0815 - acc: 0.9812 - val_loss: 0.0917 - val_acc: 0.9753\n",
      "Epoch 149/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0807 - acc: 0.9811Epoch 00148: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0809 - acc: 0.9811 - val_loss: 0.0917 - val_acc: 0.9739\n",
      "Epoch 150/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0802 - acc: 0.9815Epoch 00149: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0803 - acc: 0.9815 - val_loss: 0.0923 - val_acc: 0.9773\n",
      "Epoch 151/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9816Epoch 00150: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0805 - acc: 0.9815 - val_loss: 0.0920 - val_acc: 0.9748\n",
      "Epoch 152/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9821Epoch 00151: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0800 - acc: 0.9821 - val_loss: 0.0922 - val_acc: 0.9764\n",
      "Epoch 153/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9817Epoch 00152: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0794 - acc: 0.9816 - val_loss: 0.0948 - val_acc: 0.9746\n",
      "Epoch 154/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9821Epoch 00153: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0786 - acc: 0.9822 - val_loss: 0.0927 - val_acc: 0.9737\n",
      "Epoch 155/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0791 - acc: 0.9817Epoch 00154: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0790 - acc: 0.9818 - val_loss: 0.0949 - val_acc: 0.9737\n",
      "Epoch 156/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9821Epoch 00155: val_loss improved from 0.09172 to 0.09044, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0786 - acc: 0.9822 - val_loss: 0.0904 - val_acc: 0.9755\n",
      "Epoch 157/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0787 - acc: 0.9820Epoch 00156: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0786 - acc: 0.9820 - val_loss: 0.0906 - val_acc: 0.9755\n",
      "Epoch 158/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0779 - acc: 0.9824Epoch 00157: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0779 - acc: 0.9824 - val_loss: 0.0930 - val_acc: 0.9737\n",
      "Epoch 159/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9822Epoch 00158: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0776 - acc: 0.9822 - val_loss: 0.0915 - val_acc: 0.9755\n",
      "Epoch 160/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9826Epoch 00159: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0776 - acc: 0.9825 - val_loss: 0.0905 - val_acc: 0.9759\n",
      "Epoch 161/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0769 - acc: 0.9828Epoch 00160: val_loss improved from 0.09044 to 0.08890, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0769 - acc: 0.9828 - val_loss: 0.0889 - val_acc: 0.9761\n",
      "Epoch 162/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0765 - acc: 0.9828Epoch 00161: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0766 - acc: 0.9828 - val_loss: 0.0906 - val_acc: 0.9755\n",
      "Epoch 163/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9823Epoch 00162: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0763 - acc: 0.9823 - val_loss: 0.0905 - val_acc: 0.9761\n",
      "Epoch 164/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9831Epoch 00163: val_loss improved from 0.08890 to 0.08820, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0761 - acc: 0.9831 - val_loss: 0.0882 - val_acc: 0.9766\n",
      "Epoch 165/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9828Epoch 00164: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0761 - acc: 0.9829 - val_loss: 0.0892 - val_acc: 0.9764\n",
      "Epoch 166/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0754 - acc: 0.9825Epoch 00165: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0753 - acc: 0.9826 - val_loss: 0.0890 - val_acc: 0.9759\n",
      "Epoch 167/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9829Epoch 00166: val_loss improved from 0.08820 to 0.08761, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0755 - acc: 0.9829 - val_loss: 0.0876 - val_acc: 0.9750\n",
      "Epoch 168/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9824Epoch 00167: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0753 - acc: 0.9824 - val_loss: 0.0908 - val_acc: 0.9753\n",
      "Epoch 169/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9825Epoch 00168: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0751 - acc: 0.9825 - val_loss: 0.0886 - val_acc: 0.9755\n",
      "Epoch 170/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9834Epoch 00169: val_loss improved from 0.08761 to 0.08734, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0745 - acc: 0.9834 - val_loss: 0.0873 - val_acc: 0.9757\n",
      "Epoch 171/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0745 - acc: 0.9834Epoch 00170: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0743 - acc: 0.9834 - val_loss: 0.0878 - val_acc: 0.9755\n",
      "Epoch 172/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9833Epoch 00171: val_loss improved from 0.08734 to 0.08573, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0744 - acc: 0.9833 - val_loss: 0.0857 - val_acc: 0.9757\n",
      "Epoch 173/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9828Epoch 00172: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0737 - acc: 0.9829 - val_loss: 0.0866 - val_acc: 0.9757\n",
      "Epoch 174/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0731 - acc: 0.9837Epoch 00173: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0734 - acc: 0.9836 - val_loss: 0.0881 - val_acc: 0.9761\n",
      "Epoch 175/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0730 - acc: 0.9835Epoch 00174: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0732 - acc: 0.9835 - val_loss: 0.0887 - val_acc: 0.9753\n",
      "Epoch 176/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9837Epoch 00175: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0730 - acc: 0.9838 - val_loss: 0.0876 - val_acc: 0.9753\n",
      "Epoch 177/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9828Epoch 00176: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0726 - acc: 0.9828 - val_loss: 0.0897 - val_acc: 0.9748\n",
      "Epoch 178/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0726 - acc: 0.9836Epoch 00177: val_loss improved from 0.08573 to 0.08502, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0725 - acc: 0.9836 - val_loss: 0.0850 - val_acc: 0.9773\n",
      "Epoch 179/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9837Epoch 00178: val_loss improved from 0.08502 to 0.08489, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0721 - acc: 0.9834 - val_loss: 0.0849 - val_acc: 0.9750\n",
      "Epoch 180/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9834Epoch 00179: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0718 - acc: 0.9833 - val_loss: 0.0857 - val_acc: 0.9766\n",
      "Epoch 181/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9841Epoch 00180: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0712 - acc: 0.9840 - val_loss: 0.0867 - val_acc: 0.9757\n",
      "Epoch 182/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9841Epoch 00181: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0711 - acc: 0.9841 - val_loss: 0.0851 - val_acc: 0.9753\n",
      "Epoch 183/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9836Epoch 00182: val_loss improved from 0.08489 to 0.08409, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0710 - acc: 0.9836 - val_loss: 0.0841 - val_acc: 0.9755\n",
      "Epoch 184/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9836Epoch 00183: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0709 - acc: 0.9836 - val_loss: 0.0845 - val_acc: 0.9753\n",
      "Epoch 185/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9838Epoch 00184: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0707 - acc: 0.9838 - val_loss: 0.0844 - val_acc: 0.9773\n",
      "Epoch 186/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9841Epoch 00185: val_loss improved from 0.08409 to 0.08346, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0704 - acc: 0.9840 - val_loss: 0.0835 - val_acc: 0.9759\n",
      "Epoch 187/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0702 - acc: 0.9830Epoch 00186: val_loss improved from 0.08346 to 0.08320, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0701 - acc: 0.9829 - val_loss: 0.0832 - val_acc: 0.9768\n",
      "Epoch 188/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9837Epoch 00187: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0696 - acc: 0.9837 - val_loss: 0.0896 - val_acc: 0.9748\n",
      "Epoch 189/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9847Epoch 00188: val_loss improved from 0.08320 to 0.08240, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0695 - acc: 0.9847 - val_loss: 0.0824 - val_acc: 0.9777\n",
      "Epoch 190/400\n",
      "17776/17939 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9842Epoch 00189: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0691 - acc: 0.9842 - val_loss: 0.0829 - val_acc: 0.9770\n",
      "Epoch 191/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0693 - acc: 0.9835Epoch 00190: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0692 - acc: 0.9836 - val_loss: 0.0827 - val_acc: 0.9766\n",
      "Epoch 192/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9846Epoch 00191: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0688 - acc: 0.9846 - val_loss: 0.0866 - val_acc: 0.9759\n",
      "Epoch 193/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0690 - acc: 0.9838Epoch 00192: val_loss improved from 0.08240 to 0.08237, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0690 - acc: 0.9838 - val_loss: 0.0824 - val_acc: 0.9768\n",
      "Epoch 194/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9843Epoch 00193: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0689 - acc: 0.9843 - val_loss: 0.0828 - val_acc: 0.9757\n",
      "Epoch 195/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9852Epoch 00194: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0683 - acc: 0.9852 - val_loss: 0.0826 - val_acc: 0.9773\n",
      "Epoch 196/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9837Epoch 00195: val_loss improved from 0.08237 to 0.08218, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0684 - acc: 0.9837 - val_loss: 0.0822 - val_acc: 0.9775\n",
      "Epoch 197/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9846Epoch 00196: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0678 - acc: 0.9846 - val_loss: 0.0822 - val_acc: 0.9770\n",
      "Epoch 198/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9845Epoch 00197: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0676 - acc: 0.9844 - val_loss: 0.0824 - val_acc: 0.9768\n",
      "Epoch 199/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0674 - acc: 0.9849Epoch 00198: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0674 - acc: 0.9849 - val_loss: 0.0835 - val_acc: 0.9766\n",
      "Epoch 200/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9842Epoch 00199: val_loss improved from 0.08218 to 0.08131, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0671 - acc: 0.9842 - val_loss: 0.0813 - val_acc: 0.9777\n",
      "Epoch 201/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9847Epoch 00200: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0673 - acc: 0.9847 - val_loss: 0.0818 - val_acc: 0.9766\n",
      "Epoch 202/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9845Epoch 00201: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0669 - acc: 0.9845 - val_loss: 0.0839 - val_acc: 0.9764\n",
      "Epoch 203/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9845Epoch 00202: val_loss improved from 0.08131 to 0.08034, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0668 - acc: 0.9846 - val_loss: 0.0803 - val_acc: 0.9773\n",
      "Epoch 204/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9849Epoch 00203: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0669 - acc: 0.9848 - val_loss: 0.0820 - val_acc: 0.9764\n",
      "Epoch 205/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9846Epoch 00204: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0663 - acc: 0.9846 - val_loss: 0.0804 - val_acc: 0.9768\n",
      "Epoch 206/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9845Epoch 00205: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0655 - acc: 0.9845 - val_loss: 0.0804 - val_acc: 0.9770\n",
      "Epoch 207/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9843Epoch 00206: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0659 - acc: 0.9843 - val_loss: 0.0812 - val_acc: 0.9770\n",
      "Epoch 208/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9853Epoch 00207: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0653 - acc: 0.9853 - val_loss: 0.0818 - val_acc: 0.9773\n",
      "Epoch 209/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9849Epoch 00208: val_loss improved from 0.08034 to 0.07986, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0657 - acc: 0.9848 - val_loss: 0.0799 - val_acc: 0.9773\n",
      "Epoch 210/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9852Epoch 00209: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0649 - acc: 0.9852 - val_loss: 0.0829 - val_acc: 0.9761\n",
      "Epoch 211/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9848Epoch 00210: val_loss improved from 0.07986 to 0.07909, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0649 - acc: 0.9848 - val_loss: 0.0791 - val_acc: 0.9781\n",
      "Epoch 212/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9848Epoch 00211: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0648 - acc: 0.9848 - val_loss: 0.0800 - val_acc: 0.9761\n",
      "Epoch 213/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9847Epoch 00212: val_loss improved from 0.07909 to 0.07882, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0649 - acc: 0.9846 - val_loss: 0.0788 - val_acc: 0.9775\n",
      "Epoch 214/400\n",
      "17776/17939 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9854Epoch 00213: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0646 - acc: 0.9855 - val_loss: 0.0804 - val_acc: 0.9786\n",
      "Epoch 215/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9853Epoch 00214: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0645 - acc: 0.9852 - val_loss: 0.0805 - val_acc: 0.9773\n",
      "Epoch 216/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9854Epoch 00215: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0640 - acc: 0.9855 - val_loss: 0.0796 - val_acc: 0.9779\n",
      "Epoch 217/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9854Epoch 00216: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0641 - acc: 0.9854 - val_loss: 0.0793 - val_acc: 0.9779\n",
      "Epoch 218/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9858Epoch 00217: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0636 - acc: 0.9857 - val_loss: 0.0813 - val_acc: 0.9775\n",
      "Epoch 219/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9850Epoch 00218: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0638 - acc: 0.9850 - val_loss: 0.0791 - val_acc: 0.9790\n",
      "Epoch 220/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9851Epoch 00219: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0636 - acc: 0.9852 - val_loss: 0.0793 - val_acc: 0.9770\n",
      "Epoch 221/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9849Epoch 00220: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0633 - acc: 0.9849 - val_loss: 0.0798 - val_acc: 0.9775\n",
      "Epoch 222/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9856Epoch 00221: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0633 - acc: 0.9855 - val_loss: 0.0795 - val_acc: 0.9777\n",
      "Epoch 223/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9864Epoch 00222: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0627 - acc: 0.9863 - val_loss: 0.0812 - val_acc: 0.9770\n",
      "Epoch 224/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9856Epoch 00223: val_loss improved from 0.07882 to 0.07830, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0625 - acc: 0.9857 - val_loss: 0.0783 - val_acc: 0.9777\n",
      "Epoch 225/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9862Epoch 00224: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0623 - acc: 0.9862 - val_loss: 0.0823 - val_acc: 0.9770\n",
      "Epoch 226/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0624 - acc: 0.9857Epoch 00225: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0625 - acc: 0.9856 - val_loss: 0.0785 - val_acc: 0.9795\n",
      "Epoch 227/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9866Epoch 00226: val_loss improved from 0.07830 to 0.07821, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0620 - acc: 0.9865 - val_loss: 0.0782 - val_acc: 0.9786\n",
      "Epoch 228/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9857Epoch 00227: val_loss improved from 0.07821 to 0.07777, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0619 - acc: 0.9857 - val_loss: 0.0778 - val_acc: 0.9781\n",
      "Epoch 229/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9850Epoch 00228: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0618 - acc: 0.9851 - val_loss: 0.0806 - val_acc: 0.9775\n",
      "Epoch 230/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9859Epoch 00229: val_loss improved from 0.07777 to 0.07753, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0617 - acc: 0.9860 - val_loss: 0.0775 - val_acc: 0.9786\n",
      "Epoch 231/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9862Epoch 00230: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0616 - acc: 0.9862 - val_loss: 0.0792 - val_acc: 0.9759\n",
      "Epoch 232/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9863Epoch 00231: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0612 - acc: 0.9862 - val_loss: 0.0787 - val_acc: 0.9781\n",
      "Epoch 233/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9857Epoch 00232: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0611 - acc: 0.9857 - val_loss: 0.0803 - val_acc: 0.9766\n",
      "Epoch 234/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9862Epoch 00233: val_loss improved from 0.07753 to 0.07738, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0609 - acc: 0.9862 - val_loss: 0.0774 - val_acc: 0.9777\n",
      "Epoch 235/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9858Epoch 00234: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0609 - acc: 0.9858 - val_loss: 0.0784 - val_acc: 0.9775\n",
      "Epoch 236/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9861Epoch 00235: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0606 - acc: 0.9861 - val_loss: 0.0802 - val_acc: 0.9773\n",
      "Epoch 237/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9867Epoch 00236: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0607 - acc: 0.9867 - val_loss: 0.0781 - val_acc: 0.9784\n",
      "Epoch 238/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9861Epoch 00237: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0603 - acc: 0.9861 - val_loss: 0.0798 - val_acc: 0.9764\n",
      "Epoch 239/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9854Epoch 00238: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0603 - acc: 0.9855 - val_loss: 0.0774 - val_acc: 0.9777\n",
      "Epoch 240/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9855Epoch 00239: val_loss improved from 0.07738 to 0.07707, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0604 - acc: 0.9856 - val_loss: 0.0771 - val_acc: 0.9775\n",
      "Epoch 241/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9861Epoch 00240: val_loss improved from 0.07707 to 0.07699, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0601 - acc: 0.9862 - val_loss: 0.0770 - val_acc: 0.9779\n",
      "Epoch 242/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9858Epoch 00241: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0598 - acc: 0.9857 - val_loss: 0.0781 - val_acc: 0.9786\n",
      "Epoch 243/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9867Epoch 00242: val_loss improved from 0.07699 to 0.07691, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0599 - acc: 0.9867 - val_loss: 0.0769 - val_acc: 0.9786\n",
      "Epoch 244/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9859Epoch 00243: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0595 - acc: 0.9859 - val_loss: 0.0780 - val_acc: 0.9790\n",
      "Epoch 245/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9871Epoch 00244: val_loss improved from 0.07691 to 0.07637, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0594 - acc: 0.9870 - val_loss: 0.0764 - val_acc: 0.9790\n",
      "Epoch 246/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9861Epoch 00245: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0593 - acc: 0.9861 - val_loss: 0.0772 - val_acc: 0.9775\n",
      "Epoch 247/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9863Epoch 00246: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0588 - acc: 0.9862 - val_loss: 0.0772 - val_acc: 0.9786\n",
      "Epoch 248/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9865Epoch 00247: val_loss improved from 0.07637 to 0.07561, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0591 - acc: 0.9865 - val_loss: 0.0756 - val_acc: 0.9788\n",
      "Epoch 249/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9865Epoch 00248: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0586 - acc: 0.9865 - val_loss: 0.0777 - val_acc: 0.9773\n",
      "Epoch 250/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9866Epoch 00249: val_loss improved from 0.07561 to 0.07551, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0584 - acc: 0.9864 - val_loss: 0.0755 - val_acc: 0.9773\n",
      "Epoch 251/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9858Epoch 00250: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0585 - acc: 0.9858 - val_loss: 0.0759 - val_acc: 0.9777\n",
      "Epoch 252/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9868Epoch 00251: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0581 - acc: 0.9868 - val_loss: 0.0802 - val_acc: 0.9781\n",
      "Epoch 253/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9866Epoch 00252: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0582 - acc: 0.9867 - val_loss: 0.0778 - val_acc: 0.9770\n",
      "Epoch 254/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9870Epoch 00253: val_loss improved from 0.07551 to 0.07441, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0578 - acc: 0.9869 - val_loss: 0.0744 - val_acc: 0.9793\n",
      "Epoch 255/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9869Epoch 00254: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0578 - acc: 0.9868 - val_loss: 0.0755 - val_acc: 0.9788\n",
      "Epoch 256/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9871Epoch 00255: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0579 - acc: 0.9870 - val_loss: 0.0760 - val_acc: 0.9781\n",
      "Epoch 257/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9866Epoch 00256: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0578 - acc: 0.9866 - val_loss: 0.0777 - val_acc: 0.9759\n",
      "Epoch 258/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9869Epoch 00257: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0576 - acc: 0.9870 - val_loss: 0.0756 - val_acc: 0.9784\n",
      "Epoch 259/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9866Epoch 00258: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0575 - acc: 0.9866 - val_loss: 0.0780 - val_acc: 0.9779\n",
      "Epoch 260/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9866Epoch 00259: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0572 - acc: 0.9866 - val_loss: 0.0772 - val_acc: 0.9781\n",
      "Epoch 261/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9864Epoch 00260: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0570 - acc: 0.9864 - val_loss: 0.0772 - val_acc: 0.9761\n",
      "Epoch 262/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9872Epoch 00261: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0568 - acc: 0.9872 - val_loss: 0.0745 - val_acc: 0.9793\n",
      "Epoch 263/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0567 - acc: 0.9868Epoch 00262: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0567 - acc: 0.9868 - val_loss: 0.0755 - val_acc: 0.9781\n",
      "Epoch 264/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9870Epoch 00263: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0567 - acc: 0.9871 - val_loss: 0.0748 - val_acc: 0.9784\n",
      "Epoch 265/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0569 - acc: 0.9867Epoch 00264: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0568 - acc: 0.9867 - val_loss: 0.0744 - val_acc: 0.9781\n",
      "Epoch 266/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9867Epoch 00265: val_loss improved from 0.07441 to 0.07418, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0566 - acc: 0.9867 - val_loss: 0.0742 - val_acc: 0.9781\n",
      "Epoch 267/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9871Epoch 00266: val_loss improved from 0.07418 to 0.07378, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0562 - acc: 0.9872 - val_loss: 0.0738 - val_acc: 0.9777\n",
      "Epoch 268/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9869Epoch 00267: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0564 - acc: 0.9870 - val_loss: 0.0761 - val_acc: 0.9775\n",
      "Epoch 269/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9869Epoch 00268: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0561 - acc: 0.9868 - val_loss: 0.0738 - val_acc: 0.9784\n",
      "Epoch 270/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9864Epoch 00269: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0561 - acc: 0.9863 - val_loss: 0.0752 - val_acc: 0.9784\n",
      "Epoch 271/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9873Epoch 00270: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0556 - acc: 0.9873 - val_loss: 0.0773 - val_acc: 0.9759\n",
      "Epoch 272/400\n",
      "17776/17939 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9872Epoch 00271: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0560 - acc: 0.9872 - val_loss: 0.0745 - val_acc: 0.9790\n",
      "Epoch 273/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9871Epoch 00272: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0557 - acc: 0.9871 - val_loss: 0.0763 - val_acc: 0.9790\n",
      "Epoch 274/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9874Epoch 00273: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0555 - acc: 0.9875 - val_loss: 0.0741 - val_acc: 0.9786\n",
      "Epoch 275/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9870Epoch 00274: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0554 - acc: 0.9870 - val_loss: 0.0744 - val_acc: 0.9784\n",
      "Epoch 276/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9878Epoch 00275: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0555 - acc: 0.9878 - val_loss: 0.0743 - val_acc: 0.9790\n",
      "Epoch 277/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9874Epoch 00276: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0553 - acc: 0.9873 - val_loss: 0.0746 - val_acc: 0.9779\n",
      "Epoch 278/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9875Epoch 00277: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0550 - acc: 0.9874 - val_loss: 0.0779 - val_acc: 0.9770\n",
      "Epoch 279/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9877Epoch 00278: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0549 - acc: 0.9877 - val_loss: 0.0761 - val_acc: 0.9779\n",
      "Epoch 280/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9873Epoch 00279: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0550 - acc: 0.9873 - val_loss: 0.0742 - val_acc: 0.9793\n",
      "Epoch 281/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9874Epoch 00280: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0547 - acc: 0.9875 - val_loss: 0.0743 - val_acc: 0.9786\n",
      "Epoch 282/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9875Epoch 00281: val_loss improved from 0.07378 to 0.07377, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0545 - acc: 0.9875 - val_loss: 0.0738 - val_acc: 0.9799\n",
      "Epoch 283/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9875Epoch 00282: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0544 - acc: 0.9876 - val_loss: 0.0748 - val_acc: 0.9788\n",
      "Epoch 284/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9874Epoch 00283: val_loss improved from 0.07377 to 0.07302, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0545 - acc: 0.9874 - val_loss: 0.0730 - val_acc: 0.9793\n",
      "Epoch 285/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9875Epoch 00284: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0541 - acc: 0.9874 - val_loss: 0.0768 - val_acc: 0.9784\n",
      "Epoch 286/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9873Epoch 00285: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0541 - acc: 0.9873 - val_loss: 0.0736 - val_acc: 0.9781\n",
      "Epoch 287/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9876Epoch 00286: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0537 - acc: 0.9876 - val_loss: 0.0738 - val_acc: 0.9784\n",
      "Epoch 288/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9874Epoch 00287: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0539 - acc: 0.9873 - val_loss: 0.0772 - val_acc: 0.9761\n",
      "Epoch 289/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9875Epoch 00288: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0540 - acc: 0.9875 - val_loss: 0.0731 - val_acc: 0.9777\n",
      "Epoch 290/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9878Epoch 00289: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0535 - acc: 0.9877 - val_loss: 0.0737 - val_acc: 0.9788\n",
      "Epoch 291/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9879Epoch 00290: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0532 - acc: 0.9879 - val_loss: 0.0735 - val_acc: 0.9777\n",
      "Epoch 292/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9878Epoch 00291: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0532 - acc: 0.9877 - val_loss: 0.0741 - val_acc: 0.9786\n",
      "Epoch 293/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9871Epoch 00292: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0536 - acc: 0.9871 - val_loss: 0.0771 - val_acc: 0.9797\n",
      "Epoch 294/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9876Epoch 00293: val_loss improved from 0.07302 to 0.07276, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0534 - acc: 0.9876 - val_loss: 0.0728 - val_acc: 0.9786\n",
      "Epoch 295/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9874Epoch 00294: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0534 - acc: 0.9874 - val_loss: 0.0744 - val_acc: 0.9773\n",
      "Epoch 296/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9877Epoch 00295: val_loss improved from 0.07276 to 0.07264, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0529 - acc: 0.9877 - val_loss: 0.0726 - val_acc: 0.9797\n",
      "Epoch 297/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9878Epoch 00296: val_loss improved from 0.07264 to 0.07226, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0531 - acc: 0.9877 - val_loss: 0.0723 - val_acc: 0.9786\n",
      "Epoch 298/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9878Epoch 00297: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0529 - acc: 0.9878 - val_loss: 0.0733 - val_acc: 0.9786\n",
      "Epoch 299/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9880Epoch 00298: val_loss improved from 0.07226 to 0.07165, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0529 - acc: 0.9880 - val_loss: 0.0716 - val_acc: 0.9797\n",
      "Epoch 300/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9879Epoch 00299: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0525 - acc: 0.9880 - val_loss: 0.0751 - val_acc: 0.9779\n",
      "Epoch 301/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9875Epoch 00300: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0525 - acc: 0.9875 - val_loss: 0.0730 - val_acc: 0.9784\n",
      "Epoch 302/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9881Epoch 00301: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0524 - acc: 0.9881 - val_loss: 0.0718 - val_acc: 0.9788\n",
      "Epoch 303/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9879Epoch 00302: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0523 - acc: 0.9878 - val_loss: 0.0724 - val_acc: 0.9786\n",
      "Epoch 304/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9878Epoch 00303: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0524 - acc: 0.9878 - val_loss: 0.0723 - val_acc: 0.9786\n",
      "Epoch 305/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9883Epoch 00304: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0525 - acc: 0.9881 - val_loss: 0.0721 - val_acc: 0.9793\n",
      "Epoch 306/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9877Epoch 00305: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0520 - acc: 0.9877 - val_loss: 0.0742 - val_acc: 0.9775\n",
      "Epoch 307/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9878Epoch 00306: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0517 - acc: 0.9878 - val_loss: 0.0743 - val_acc: 0.9788\n",
      "Epoch 308/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9883Epoch 00307: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0517 - acc: 0.9883 - val_loss: 0.0752 - val_acc: 0.9775\n",
      "Epoch 309/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9879Epoch 00308: val_loss improved from 0.07165 to 0.07101, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0519 - acc: 0.9878 - val_loss: 0.0710 - val_acc: 0.9786\n",
      "Epoch 310/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9879Epoch 00309: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0518 - acc: 0.9878 - val_loss: 0.0731 - val_acc: 0.9781\n",
      "Epoch 311/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9879Epoch 00310: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0515 - acc: 0.9879 - val_loss: 0.0714 - val_acc: 0.9788\n",
      "Epoch 312/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9881Epoch 00311: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0515 - acc: 0.9881 - val_loss: 0.0729 - val_acc: 0.9808\n",
      "Epoch 313/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9876Epoch 00312: val_loss improved from 0.07101 to 0.07066, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0514 - acc: 0.9877 - val_loss: 0.0707 - val_acc: 0.9797\n",
      "Epoch 314/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9881Epoch 00313: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0510 - acc: 0.9880 - val_loss: 0.0715 - val_acc: 0.9781\n",
      "Epoch 315/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9875Epoch 00314: val_loss improved from 0.07066 to 0.07057, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0515 - acc: 0.9876 - val_loss: 0.0706 - val_acc: 0.9790\n",
      "Epoch 316/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9885Epoch 00315: val_loss improved from 0.07057 to 0.06970, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0510 - acc: 0.9886 - val_loss: 0.0697 - val_acc: 0.9799\n",
      "Epoch 317/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9881Epoch 00316: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0510 - acc: 0.9881 - val_loss: 0.0707 - val_acc: 0.9802\n",
      "Epoch 318/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9883Epoch 00317: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0510 - acc: 0.9883 - val_loss: 0.0724 - val_acc: 0.9779\n",
      "Epoch 319/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9885Epoch 00318: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0505 - acc: 0.9885 - val_loss: 0.0722 - val_acc: 0.9790\n",
      "Epoch 320/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9888Epoch 00319: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0507 - acc: 0.9889 - val_loss: 0.0713 - val_acc: 0.9790\n",
      "Epoch 321/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9876Epoch 00320: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0508 - acc: 0.9876 - val_loss: 0.0726 - val_acc: 0.9777\n",
      "Epoch 322/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9883Epoch 00321: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0504 - acc: 0.9883 - val_loss: 0.0734 - val_acc: 0.9797\n",
      "Epoch 323/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9889Epoch 00322: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0503 - acc: 0.9889 - val_loss: 0.0736 - val_acc: 0.9775\n",
      "Epoch 324/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9885Epoch 00323: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0503 - acc: 0.9885 - val_loss: 0.0724 - val_acc: 0.9788\n",
      "Epoch 325/400\n",
      "17776/17939 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9885Epoch 00324: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0504 - acc: 0.9885 - val_loss: 0.0713 - val_acc: 0.9795\n",
      "Epoch 326/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9889Epoch 00325: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0501 - acc: 0.9889 - val_loss: 0.0715 - val_acc: 0.9797\n",
      "Epoch 327/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9885Epoch 00326: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0498 - acc: 0.9886 - val_loss: 0.0727 - val_acc: 0.9788\n",
      "Epoch 328/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9883Epoch 00327: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0502 - acc: 0.9883 - val_loss: 0.0710 - val_acc: 0.9790\n",
      "Epoch 329/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9889Epoch 00328: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0498 - acc: 0.9887 - val_loss: 0.0711 - val_acc: 0.9784\n",
      "Epoch 330/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9885Epoch 00329: val_loss improved from 0.06970 to 0.06958, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0501 - acc: 0.9886 - val_loss: 0.0696 - val_acc: 0.9795\n",
      "Epoch 331/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9886Epoch 00330: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0499 - acc: 0.9885 - val_loss: 0.0696 - val_acc: 0.9808\n",
      "Epoch 332/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9888Epoch 00331: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0495 - acc: 0.9888 - val_loss: 0.0723 - val_acc: 0.9788\n",
      "Epoch 333/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9881Epoch 00332: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0496 - acc: 0.9881 - val_loss: 0.0745 - val_acc: 0.9779\n",
      "Epoch 334/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9883Epoch 00333: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0495 - acc: 0.9883 - val_loss: 0.0703 - val_acc: 0.9786\n",
      "Epoch 335/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9889Epoch 00334: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0496 - acc: 0.9888 - val_loss: 0.0740 - val_acc: 0.9773\n",
      "Epoch 336/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9886Epoch 00335: val_loss improved from 0.06958 to 0.06926, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0494 - acc: 0.9886 - val_loss: 0.0693 - val_acc: 0.9799\n",
      "Epoch 337/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9882Epoch 00336: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0492 - acc: 0.9882 - val_loss: 0.0693 - val_acc: 0.9797\n",
      "Epoch 338/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9885Epoch 00337: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0493 - acc: 0.9884 - val_loss: 0.0703 - val_acc: 0.9793\n",
      "Epoch 339/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9888Epoch 00338: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0488 - acc: 0.9888 - val_loss: 0.0697 - val_acc: 0.9795\n",
      "Epoch 340/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9890Epoch 00339: val_loss improved from 0.06926 to 0.06898, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0492 - acc: 0.9890 - val_loss: 0.0690 - val_acc: 0.9795\n",
      "Epoch 341/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9890Epoch 00340: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0488 - acc: 0.9890 - val_loss: 0.0716 - val_acc: 0.9784\n",
      "Epoch 342/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9886Epoch 00341: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0489 - acc: 0.9886 - val_loss: 0.0693 - val_acc: 0.9802\n",
      "Epoch 343/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9888Epoch 00342: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0486 - acc: 0.9889 - val_loss: 0.0703 - val_acc: 0.9781\n",
      "Epoch 344/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9891Epoch 00343: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0489 - acc: 0.9890 - val_loss: 0.0710 - val_acc: 0.9781\n",
      "Epoch 345/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9887Epoch 00344: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0486 - acc: 0.9887 - val_loss: 0.0733 - val_acc: 0.9784\n",
      "Epoch 346/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9891Epoch 00345: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0488 - acc: 0.9891 - val_loss: 0.0695 - val_acc: 0.9788\n",
      "Epoch 347/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9888Epoch 00346: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0481 - acc: 0.9888 - val_loss: 0.0698 - val_acc: 0.9784\n",
      "Epoch 348/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9891Epoch 00347: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0481 - acc: 0.9890 - val_loss: 0.0694 - val_acc: 0.9802\n",
      "Epoch 349/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9888Epoch 00348: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0485 - acc: 0.9889 - val_loss: 0.0699 - val_acc: 0.9779\n",
      "Epoch 350/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9887Epoch 00349: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0483 - acc: 0.9887 - val_loss: 0.0700 - val_acc: 0.9793\n",
      "Epoch 351/400\n",
      "17776/17939 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9889Epoch 00350: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0479 - acc: 0.9889 - val_loss: 0.0703 - val_acc: 0.9784\n",
      "Epoch 352/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9890Epoch 00351: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0480 - acc: 0.9890 - val_loss: 0.0707 - val_acc: 0.9788\n",
      "Epoch 353/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9887Epoch 00352: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0479 - acc: 0.9887 - val_loss: 0.0707 - val_acc: 0.9786\n",
      "Epoch 354/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9896Epoch 00353: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0481 - acc: 0.9895 - val_loss: 0.0698 - val_acc: 0.9790\n",
      "Epoch 355/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9892Epoch 00354: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0477 - acc: 0.9892 - val_loss: 0.0714 - val_acc: 0.9793\n",
      "Epoch 356/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9892Epoch 00355: val_loss improved from 0.06898 to 0.06879, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0478 - acc: 0.9892 - val_loss: 0.0688 - val_acc: 0.9802\n",
      "Epoch 357/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9889Epoch 00356: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0476 - acc: 0.9890 - val_loss: 0.0698 - val_acc: 0.9797\n",
      "Epoch 358/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9889Epoch 00357: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0474 - acc: 0.9890 - val_loss: 0.0696 - val_acc: 0.9799\n",
      "Epoch 359/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9893Epoch 00358: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0474 - acc: 0.9893 - val_loss: 0.0689 - val_acc: 0.9799\n",
      "Epoch 360/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9891Epoch 00359: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0474 - acc: 0.9891 - val_loss: 0.0736 - val_acc: 0.9770\n",
      "Epoch 361/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9895Epoch 00360: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0477 - acc: 0.9895 - val_loss: 0.0703 - val_acc: 0.9790\n",
      "Epoch 362/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9893Epoch 00361: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0472 - acc: 0.9893 - val_loss: 0.0703 - val_acc: 0.9790\n",
      "Epoch 363/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9896Epoch 00362: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0473 - acc: 0.9896 - val_loss: 0.0703 - val_acc: 0.9784\n",
      "Epoch 364/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9892Epoch 00363: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0472 - acc: 0.9892 - val_loss: 0.0728 - val_acc: 0.9786\n",
      "Epoch 365/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9890Epoch 00364: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0473 - acc: 0.9891 - val_loss: 0.0702 - val_acc: 0.9797\n",
      "Epoch 366/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9894Epoch 00365: val_loss improved from 0.06879 to 0.06876, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 7s - loss: 0.0472 - acc: 0.9894 - val_loss: 0.0688 - val_acc: 0.9777\n",
      "Epoch 367/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9897Epoch 00366: val_loss improved from 0.06876 to 0.06776, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning\n",
      "17939/17939 [==============================] - 6s - loss: 0.0469 - acc: 0.9897 - val_loss: 0.0678 - val_acc: 0.9806\n",
      "Epoch 368/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9894Epoch 00367: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0467 - acc: 0.9894 - val_loss: 0.0694 - val_acc: 0.9793\n",
      "Epoch 369/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9894Epoch 00368: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0467 - acc: 0.9895 - val_loss: 0.0683 - val_acc: 0.9802\n",
      "Epoch 370/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9891Epoch 00369: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0470 - acc: 0.9891 - val_loss: 0.0699 - val_acc: 0.9790\n",
      "Epoch 371/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9894Epoch 00370: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0467 - acc: 0.9894 - val_loss: 0.0737 - val_acc: 0.9788\n",
      "Epoch 372/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9898Epoch 00371: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0467 - acc: 0.9898 - val_loss: 0.0706 - val_acc: 0.9790\n",
      "Epoch 373/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9898Epoch 00372: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0462 - acc: 0.9898 - val_loss: 0.0692 - val_acc: 0.9790\n",
      "Epoch 374/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9892Epoch 00373: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0466 - acc: 0.9892 - val_loss: 0.0698 - val_acc: 0.9788\n",
      "Epoch 375/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9892Epoch 00374: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0462 - acc: 0.9892 - val_loss: 0.0689 - val_acc: 0.9793\n",
      "Epoch 376/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9888Epoch 00375: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0463 - acc: 0.9887 - val_loss: 0.0705 - val_acc: 0.9784\n",
      "Epoch 377/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9892Epoch 00376: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0464 - acc: 0.9892 - val_loss: 0.0683 - val_acc: 0.9808\n",
      "Epoch 378/400\n",
      "17824/17939 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9892Epoch 00377: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0465 - acc: 0.9893 - val_loss: 0.0682 - val_acc: 0.9795\n",
      "Epoch 379/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9892Epoch 00378: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0461 - acc: 0.9892 - val_loss: 0.0681 - val_acc: 0.9808\n",
      "Epoch 380/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9893Epoch 00379: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0461 - acc: 0.9893 - val_loss: 0.0693 - val_acc: 0.9797\n",
      "Epoch 381/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9893Epoch 00380: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0460 - acc: 0.9892 - val_loss: 0.0685 - val_acc: 0.9795\n",
      "Epoch 382/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9898Epoch 00381: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0461 - acc: 0.9898 - val_loss: 0.0704 - val_acc: 0.9788\n",
      "Epoch 383/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9895Epoch 00382: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0458 - acc: 0.9895 - val_loss: 0.0682 - val_acc: 0.9795\n",
      "Epoch 384/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9896Epoch 00383: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0461 - acc: 0.9897 - val_loss: 0.0688 - val_acc: 0.9781\n",
      "Epoch 385/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9899Epoch 00384: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0458 - acc: 0.9899 - val_loss: 0.0685 - val_acc: 0.9804\n",
      "Epoch 386/400\n",
      "17840/17939 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9893Epoch 00385: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0456 - acc: 0.9894 - val_loss: 0.0687 - val_acc: 0.9797\n",
      "Epoch 387/400\n",
      "17888/17939 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9896Epoch 00386: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0457 - acc: 0.9896 - val_loss: 0.0682 - val_acc: 0.9790\n",
      "Epoch 388/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9898Epoch 00387: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0458 - acc: 0.9898 - val_loss: 0.0687 - val_acc: 0.9795\n",
      "Epoch 389/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9895Epoch 00388: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0455 - acc: 0.9895 - val_loss: 0.0683 - val_acc: 0.9788\n",
      "Epoch 390/400\n",
      "17808/17939 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9896Epoch 00389: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0454 - acc: 0.9896 - val_loss: 0.0730 - val_acc: 0.9799\n",
      "Epoch 391/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9893Epoch 00390: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0452 - acc: 0.9892 - val_loss: 0.0714 - val_acc: 0.9790\n",
      "Epoch 392/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9896Epoch 00391: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0452 - acc: 0.9896 - val_loss: 0.0704 - val_acc: 0.9781\n",
      "Epoch 393/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9894Epoch 00392: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0452 - acc: 0.9894 - val_loss: 0.0689 - val_acc: 0.9786\n",
      "Epoch 394/400\n",
      "17856/17939 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9898Epoch 00393: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0453 - acc: 0.9898 - val_loss: 0.0689 - val_acc: 0.9802\n",
      "Epoch 395/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9898Epoch 00394: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0451 - acc: 0.9898 - val_loss: 0.0692 - val_acc: 0.9790\n",
      "Epoch 396/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9897Epoch 00395: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0450 - acc: 0.9897 - val_loss: 0.0691 - val_acc: 0.9786\n",
      "Epoch 397/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9900Epoch 00396: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0452 - acc: 0.9900 - val_loss: 0.0683 - val_acc: 0.9795\n",
      "Epoch 398/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9899Epoch 00397: val_loss did not improve\n",
      "17939/17939 [==============================] - 7s - loss: 0.0450 - acc: 0.9899 - val_loss: 0.0693 - val_acc: 0.9799\n",
      "Epoch 399/400\n",
      "17792/17939 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9903Epoch 00398: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0449 - acc: 0.9904 - val_loss: 0.0688 - val_acc: 0.9793\n",
      "Epoch 400/400\n",
      "17872/17939 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9895Epoch 00399: val_loss did not improve\n",
      "17939/17939 [==============================] - 6s - loss: 0.0447 - acc: 0.9895 - val_loss: 0.0703 - val_acc: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c607953c8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5_transfer_learning', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(bottleneck_features_train_VGG16, train_targets, \n",
    "          validation_data=(bottleneck_features_valid_VGG16, valid_targets),\n",
    "          epochs=400, batch_size=16, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5_transfer_learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "The model is tried on the test dataset of driver images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "#VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "VGG16_predictions = [VGG16_model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in bottleneck_features_test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "#test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "#print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_subm = np.column_stack((np.asarray(test_files_final), np.asarray(VGG16_predictions,dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Model Architecture2\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We have added a Flatten layer and two fully connected layers.The last fully connected layer contains one node for each driver category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features_train2_VGG16 = np.load('bottleneck_features/bottleneck_features_train_VGG16.npy')\n",
    "bottleneck_features_valid2_VGG16 = np.load('bottleneck_features/bottleneck_features_valid_VGG16.npy')\n",
    "bottleneck_features_test2_VGG16 = np.load('bottleneck_features/bottleneck_features_test_VGG16.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               12544500  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 12,549,510\n",
      "Trainable params: 12,549,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "VGG16_model2 = Sequential()\n",
    "VGG16_model2.add(Flatten(input_shape=bottleneck_features_train2_VGG16.shape[1:]))\n",
    "VGG16_model2.add(Dense(500, activation='relu',kernel_initializer='glorot_normal'))\n",
    "VGG16_model2.add(Dropout(0.5))\n",
    "VGG16_model2.add(Dense(10, activation='softmax',kernel_initializer='glorot_normal'))\n",
    "\n",
    "VGG16_model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model2.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17939 samples, validate on 4485 samples\n",
      "Epoch 1/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 3.3116 - acc: 0.6014Epoch 00000: val_loss improved from inf to 0.07532, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 3.3054 - acc: 0.6021 - val_loss: 0.0753 - val_acc: 0.9775\n",
      "Epoch 2/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.3763 - acc: 0.8923Epoch 00001: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.3769 - acc: 0.8922 - val_loss: 0.0879 - val_acc: 0.9784\n",
      "Epoch 3/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.2845 - acc: 0.9284Epoch 00002: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.2845 - acc: 0.9284 - val_loss: 0.1422 - val_acc: 0.9625\n",
      "Epoch 4/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.2572 - acc: 0.9436Epoch 00003: val_loss improved from 0.07532 to 0.04608, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.2574 - acc: 0.9436 - val_loss: 0.0461 - val_acc: 0.9931\n",
      "Epoch 5/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.2278 - acc: 0.9512Epoch 00004: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.2277 - acc: 0.9512 - val_loss: 0.0968 - val_acc: 0.9826\n",
      "Epoch 6/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9587Epoch 00005: val_loss improved from 0.04608 to 0.03882, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.2004 - acc: 0.9587 - val_loss: 0.0388 - val_acc: 0.9938\n",
      "Epoch 7/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1971 - acc: 0.9619Epoch 00006: val_loss improved from 0.03882 to 0.03184, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.1970 - acc: 0.9619 - val_loss: 0.0318 - val_acc: 0.9949\n",
      "Epoch 8/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9653Epoch 00007: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1899 - acc: 0.9653 - val_loss: 0.0464 - val_acc: 0.9933\n",
      "Epoch 9/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.9680Epoch 00008: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1784 - acc: 0.9679 - val_loss: 0.0419 - val_acc: 0.9946\n",
      "Epoch 10/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1876 - acc: 0.9701Epoch 00009: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1876 - acc: 0.9701 - val_loss: 0.0361 - val_acc: 0.9951\n",
      "Epoch 11/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1634 - acc: 0.9734Epoch 00010: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1632 - acc: 0.9734 - val_loss: 0.0390 - val_acc: 0.9940\n",
      "Epoch 12/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.9727Epoch 00011: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1780 - acc: 0.9726 - val_loss: 0.0519 - val_acc: 0.9922\n",
      "Epoch 13/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.9743Epoch 00012: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1560 - acc: 0.9743 - val_loss: 0.0360 - val_acc: 0.9935\n",
      "Epoch 14/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1606 - acc: 0.9739Epoch 00013: val_loss improved from 0.03184 to 0.02911, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.1613 - acc: 0.9739 - val_loss: 0.0291 - val_acc: 0.9960\n",
      "Epoch 15/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9765Epoch 00014: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1652 - acc: 0.9765 - val_loss: 0.0361 - val_acc: 0.9955\n",
      "Epoch 16/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.9774Epoch 00015: val_loss improved from 0.02911 to 0.02873, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.1483 - acc: 0.9774 - val_loss: 0.0287 - val_acc: 0.9962\n",
      "Epoch 17/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1602 - acc: 0.9770Epoch 00016: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1601 - acc: 0.9770 - val_loss: 0.0530 - val_acc: 0.9942\n",
      "Epoch 18/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.9784Epoch 00017: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1463 - acc: 0.9784 - val_loss: 0.0295 - val_acc: 0.9967\n",
      "Epoch 19/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9793Epoch 00018: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1478 - acc: 0.9793 - val_loss: 0.0431 - val_acc: 0.9958\n",
      "Epoch 20/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1343 - acc: 0.9809Epoch 00019: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1343 - acc: 0.9809 - val_loss: 0.0386 - val_acc: 0.9949\n",
      "Epoch 21/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.9800Epoch 00020: val_loss improved from 0.02873 to 0.02025, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.1493 - acc: 0.9800 - val_loss: 0.0203 - val_acc: 0.9975\n",
      "Epoch 22/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.9816Epoch 00021: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1368 - acc: 0.9816 - val_loss: 0.0444 - val_acc: 0.9946\n",
      "Epoch 23/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1318 - acc: 0.9832Epoch 00022: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1316 - acc: 0.9833 - val_loss: 0.0289 - val_acc: 0.9975\n",
      "Epoch 24/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9837Epoch 00023: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1320 - acc: 0.9837 - val_loss: 0.0275 - val_acc: 0.9975\n",
      "Epoch 25/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.9831Epoch 00024: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1429 - acc: 0.9831 - val_loss: 0.0246 - val_acc: 0.9969\n",
      "Epoch 26/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1293 - acc: 0.9837Epoch 00025: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1293 - acc: 0.9837 - val_loss: 0.0203 - val_acc: 0.9980\n",
      "Epoch 27/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9830Epoch 00026: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1386 - acc: 0.9830 - val_loss: 0.0317 - val_acc: 0.9960\n",
      "Epoch 28/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1159 - acc: 0.9857Epoch 00027: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1157 - acc: 0.9857 - val_loss: 0.0286 - val_acc: 0.9971\n",
      "Epoch 29/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9842Epoch 00028: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1262 - acc: 0.9842 - val_loss: 0.0231 - val_acc: 0.9982\n",
      "Epoch 30/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1232 - acc: 0.9858Epoch 00029: val_loss improved from 0.02025 to 0.01709, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.1232 - acc: 0.9858 - val_loss: 0.0171 - val_acc: 0.9982\n",
      "Epoch 31/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9849Epoch 00030: val_loss improved from 0.01709 to 0.01627, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.1263 - acc: 0.9849 - val_loss: 0.0163 - val_acc: 0.9984\n",
      "Epoch 32/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1142 - acc: 0.9861Epoch 00031: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1141 - acc: 0.9861 - val_loss: 0.0506 - val_acc: 0.9946\n",
      "Epoch 33/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1073 - acc: 0.9863Epoch 00032: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1072 - acc: 0.9863 - val_loss: 0.0257 - val_acc: 0.9971\n",
      "Epoch 34/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.9851Epoch 00033: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1427 - acc: 0.9851 - val_loss: 0.0429 - val_acc: 0.9949\n",
      "Epoch 35/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1138 - acc: 0.9862Epoch 00034: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1137 - acc: 0.9862 - val_loss: 0.0182 - val_acc: 0.9987\n",
      "Epoch 36/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1129 - acc: 0.9878Epoch 00035: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1139 - acc: 0.9877 - val_loss: 0.0266 - val_acc: 0.9971\n",
      "Epoch 37/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9852Epoch 00036: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1457 - acc: 0.9852 - val_loss: 0.0225 - val_acc: 0.9982\n",
      "Epoch 38/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9892Epoch 00037: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0910 - acc: 0.9892 - val_loss: 0.0255 - val_acc: 0.9978\n",
      "Epoch 39/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1042 - acc: 0.9884Epoch 00038: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1041 - acc: 0.9885 - val_loss: 0.0222 - val_acc: 0.9978\n",
      "Epoch 40/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0983 - acc: 0.9883Epoch 00039: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0982 - acc: 0.9883 - val_loss: 0.0366 - val_acc: 0.9960\n",
      "Epoch 41/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0988 - acc: 0.9897Epoch 00040: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0987 - acc: 0.9897 - val_loss: 0.0316 - val_acc: 0.9971\n",
      "Epoch 42/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1192 - acc: 0.9878Epoch 00041: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1191 - acc: 0.9878 - val_loss: 0.0304 - val_acc: 0.9969\n",
      "Epoch 43/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9890Epoch 00042: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0911 - acc: 0.9890 - val_loss: 0.0333 - val_acc: 0.9967\n",
      "Epoch 44/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0960 - acc: 0.9882Epoch 00043: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0959 - acc: 0.9882 - val_loss: 0.0324 - val_acc: 0.9975\n",
      "Epoch 45/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9886Epoch 00044: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0986 - acc: 0.9886 - val_loss: 0.0256 - val_acc: 0.9980\n",
      "Epoch 46/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9895Epoch 00045: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0841 - acc: 0.9895 - val_loss: 0.0214 - val_acc: 0.9987\n",
      "Epoch 47/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1024 - acc: 0.9883Epoch 00046: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1033 - acc: 0.9882 - val_loss: 0.0200 - val_acc: 0.9975\n",
      "Epoch 48/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.1126 - acc: 0.9883Epoch 00047: val_loss improved from 0.01627 to 0.01460, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.1125 - acc: 0.9883 - val_loss: 0.0146 - val_acc: 0.9984\n",
      "Epoch 49/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0926 - acc: 0.9895Epoch 00048: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0924 - acc: 0.9895 - val_loss: 0.0271 - val_acc: 0.9975\n",
      "Epoch 50/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9914Epoch 00049: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0850 - acc: 0.9914 - val_loss: 0.0279 - val_acc: 0.9978\n",
      "Epoch 51/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.1206 - acc: 0.9874Epoch 00050: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1203 - acc: 0.9875 - val_loss: 0.0319 - val_acc: 0.9971\n",
      "Epoch 52/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.1094 - acc: 0.9887Epoch 00051: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.1094 - acc: 0.9887 - val_loss: 0.0455 - val_acc: 0.9962\n",
      "Epoch 53/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0942 - acc: 0.9902Epoch 00052: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0940 - acc: 0.9902 - val_loss: 0.0194 - val_acc: 0.9982\n",
      "Epoch 54/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9893Epoch 00053: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0988 - acc: 0.9893 - val_loss: 0.0152 - val_acc: 0.9984\n",
      "Epoch 55/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9916Epoch 00054: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0800 - acc: 0.9916 - val_loss: 0.0307 - val_acc: 0.9973\n",
      "Epoch 56/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0878 - acc: 0.9905Epoch 00055: val_loss improved from 0.01460 to 0.01215, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.0878 - acc: 0.9905 - val_loss: 0.0122 - val_acc: 0.9987\n",
      "Epoch 57/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0959 - acc: 0.9895Epoch 00056: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0958 - acc: 0.9895 - val_loss: 0.0189 - val_acc: 0.9973\n",
      "Epoch 58/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9911Epoch 00057: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0876 - acc: 0.9911 - val_loss: 0.0206 - val_acc: 0.9978\n",
      "Epoch 59/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0893 - acc: 0.9907Epoch 00058: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0891 - acc: 0.9907 - val_loss: 0.0250 - val_acc: 0.9978\n",
      "Epoch 60/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9907Epoch 00059: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0906 - acc: 0.9907 - val_loss: 0.0252 - val_acc: 0.9971\n",
      "Epoch 61/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9930Epoch 00060: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0592 - acc: 0.9930 - val_loss: 0.0200 - val_acc: 0.9975\n",
      "Epoch 62/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9913Epoch 00061: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0834 - acc: 0.9913 - val_loss: 0.0170 - val_acc: 0.9984\n",
      "Epoch 63/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9928Epoch 00062: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0656 - acc: 0.9928 - val_loss: 0.0137 - val_acc: 0.9982\n",
      "Epoch 64/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9919Epoch 00063: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0836 - acc: 0.9919 - val_loss: 0.0144 - val_acc: 0.9989\n",
      "Epoch 65/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9922Epoch 00064: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0743 - acc: 0.9922 - val_loss: 0.0249 - val_acc: 0.9982\n",
      "Epoch 66/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0781 - acc: 0.9914Epoch 00065: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0789 - acc: 0.9913 - val_loss: 0.0269 - val_acc: 0.9978\n",
      "Epoch 67/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0741 - acc: 0.9926Epoch 00066: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0740 - acc: 0.9926 - val_loss: 0.0278 - val_acc: 0.9975\n",
      "Epoch 68/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0972 - acc: 0.9902Epoch 00067: val_loss improved from 0.01215 to 0.00922, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.0970 - acc: 0.9902 - val_loss: 0.0092 - val_acc: 0.9989\n",
      "Epoch 69/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9921Epoch 00068: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0720 - acc: 0.9921 - val_loss: 0.0192 - val_acc: 0.9982\n",
      "Epoch 70/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9917Epoch 00069: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0785 - acc: 0.9917 - val_loss: 0.0223 - val_acc: 0.9980\n",
      "Epoch 71/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0780 - acc: 0.9921Epoch 00070: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0780 - acc: 0.9921 - val_loss: 0.0329 - val_acc: 0.9971\n",
      "Epoch 72/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0740 - acc: 0.9919Epoch 00071: val_loss did not improve\n",
      "17939/17939 [==============================] - 20s - loss: 0.0739 - acc: 0.9919 - val_loss: 0.0210 - val_acc: 0.9984\n",
      "Epoch 73/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0805 - acc: 0.9919Epoch 00072: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0804 - acc: 0.9919 - val_loss: 0.0216 - val_acc: 0.9984\n",
      "Epoch 74/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0859 - acc: 0.9913Epoch 00073: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0857 - acc: 0.9914 - val_loss: 0.0158 - val_acc: 0.9984\n",
      "Epoch 75/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0816 - acc: 0.9912Epoch 00074: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0816 - acc: 0.9912 - val_loss: 0.0222 - val_acc: 0.9982\n",
      "Epoch 76/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9921Epoch 00075: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0734 - acc: 0.9921 - val_loss: 0.0258 - val_acc: 0.9978\n",
      "Epoch 77/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9916Epoch 00076: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0721 - acc: 0.9916 - val_loss: 0.0217 - val_acc: 0.9982\n",
      "Epoch 78/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9923Epoch 00077: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0733 - acc: 0.9923 - val_loss: 0.0142 - val_acc: 0.9982\n",
      "Epoch 79/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0908 - acc: 0.9914Epoch 00078: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0907 - acc: 0.9913 - val_loss: 0.0236 - val_acc: 0.9980\n",
      "Epoch 80/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9931Epoch 00079: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0601 - acc: 0.9931 - val_loss: 0.0174 - val_acc: 0.9982\n",
      "Epoch 81/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9951Epoch 00080: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0446 - acc: 0.9952 - val_loss: 0.0226 - val_acc: 0.9982\n",
      "Epoch 82/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9915Epoch 00081: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0798 - acc: 0.9915 - val_loss: 0.0144 - val_acc: 0.9989\n",
      "Epoch 83/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9936Epoch 00082: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0617 - acc: 0.9936 - val_loss: 0.0388 - val_acc: 0.9967\n",
      "Epoch 84/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0632 - acc: 0.9931Epoch 00083: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0631 - acc: 0.9931 - val_loss: 0.0385 - val_acc: 0.9969\n",
      "Epoch 85/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9912Epoch 00084: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0919 - acc: 0.9912 - val_loss: 0.0201 - val_acc: 0.9987\n",
      "Epoch 86/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9934Epoch 00085: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0646 - acc: 0.9934 - val_loss: 0.0313 - val_acc: 0.9971\n",
      "Epoch 87/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9927Epoch 00086: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0737 - acc: 0.9927 - val_loss: 0.0167 - val_acc: 0.9987\n",
      "Epoch 88/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9941Epoch 00087: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0586 - acc: 0.9941 - val_loss: 0.0162 - val_acc: 0.9987\n",
      "Epoch 89/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9939Epoch 00088: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0491 - acc: 0.9939 - val_loss: 0.0181 - val_acc: 0.9984\n",
      "Epoch 90/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9933Epoch 00089: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0661 - acc: 0.9933 - val_loss: 0.0268 - val_acc: 0.9973\n",
      "Epoch 91/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9934Epoch 00090: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0644 - acc: 0.9934 - val_loss: 0.0215 - val_acc: 0.9982\n",
      "Epoch 92/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9934Epoch 00091: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0605 - acc: 0.9934 - val_loss: 0.0185 - val_acc: 0.9984\n",
      "Epoch 93/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9938Epoch 00092: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0584 - acc: 0.9938 - val_loss: 0.0296 - val_acc: 0.9971\n",
      "Epoch 94/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9938Epoch 00093: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0573 - acc: 0.9938 - val_loss: 0.0243 - val_acc: 0.9978\n",
      "Epoch 95/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9927Epoch 00094: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0698 - acc: 0.9928 - val_loss: 0.0240 - val_acc: 0.9980\n",
      "Epoch 96/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9938Epoch 00095: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0524 - acc: 0.9938 - val_loss: 0.0298 - val_acc: 0.9973\n",
      "Epoch 97/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0751 - acc: 0.9931Epoch 00096: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0750 - acc: 0.9931 - val_loss: 0.0189 - val_acc: 0.9984\n",
      "Epoch 98/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9932Epoch 00097: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0591 - acc: 0.9932 - val_loss: 0.0220 - val_acc: 0.9984\n",
      "Epoch 99/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9942Epoch 00098: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0575 - acc: 0.9942 - val_loss: 0.0098 - val_acc: 0.9991\n",
      "Epoch 100/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9939Epoch 00099: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0562 - acc: 0.9939 - val_loss: 0.0299 - val_acc: 0.9973\n",
      "Epoch 101/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9929Epoch 00100: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0740 - acc: 0.9929 - val_loss: 0.0262 - val_acc: 0.9971\n",
      "Epoch 102/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9951Epoch 00101: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0480 - acc: 0.9951 - val_loss: 0.0205 - val_acc: 0.9978\n",
      "Epoch 103/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9944Epoch 00102: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0563 - acc: 0.9943 - val_loss: 0.0216 - val_acc: 0.9980\n",
      "Epoch 104/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9963Epoch 00103: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0381 - acc: 0.9963 - val_loss: 0.0216 - val_acc: 0.9987\n",
      "Epoch 105/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9949Epoch 00104: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0505 - acc: 0.9949 - val_loss: 0.0230 - val_acc: 0.9978\n",
      "Epoch 106/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0652 - acc: 0.9938Epoch 00105: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0651 - acc: 0.9938 - val_loss: 0.0308 - val_acc: 0.9975\n",
      "Epoch 107/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9945Epoch 00106: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0516 - acc: 0.9945 - val_loss: 0.0272 - val_acc: 0.9975\n",
      "Epoch 108/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9956Epoch 00107: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0441 - acc: 0.9957 - val_loss: 0.0215 - val_acc: 0.9982\n",
      "Epoch 109/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9948Epoch 00108: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0440 - acc: 0.9948 - val_loss: 0.0174 - val_acc: 0.9982\n",
      "Epoch 110/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9949Epoch 00109: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0565 - acc: 0.9949 - val_loss: 0.0332 - val_acc: 0.9973\n",
      "Epoch 111/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9954Epoch 00110: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0439 - acc: 0.9954 - val_loss: 0.0185 - val_acc: 0.9984\n",
      "Epoch 112/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9945Epoch 00111: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0509 - acc: 0.9945 - val_loss: 0.0140 - val_acc: 0.9984\n",
      "Epoch 113/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9941Epoch 00112: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0611 - acc: 0.9941 - val_loss: 0.0155 - val_acc: 0.9989\n",
      "Epoch 114/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9931Epoch 00113: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0694 - acc: 0.9931 - val_loss: 0.0279 - val_acc: 0.9980\n",
      "Epoch 115/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9929Epoch 00114: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0711 - acc: 0.9929 - val_loss: 0.0232 - val_acc: 0.9980\n",
      "Epoch 116/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9949Epoch 00115: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0566 - acc: 0.9949 - val_loss: 0.0321 - val_acc: 0.9973\n",
      "Epoch 117/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0646 - acc: 0.9936Epoch 00116: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0645 - acc: 0.9936 - val_loss: 0.0217 - val_acc: 0.9982\n",
      "Epoch 118/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9943Epoch 00117: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0549 - acc: 0.9943 - val_loss: 0.0166 - val_acc: 0.9984\n",
      "Epoch 119/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9955Epoch 00118: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0395 - acc: 0.9955 - val_loss: 0.0102 - val_acc: 0.9991\n",
      "Epoch 120/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9945Epoch 00119: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0564 - acc: 0.9945 - val_loss: 0.0098 - val_acc: 0.9991\n",
      "Epoch 121/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9941Epoch 00120: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0620 - acc: 0.9941 - val_loss: 0.0216 - val_acc: 0.9982\n",
      "Epoch 122/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9934Epoch 00121: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0669 - acc: 0.9934 - val_loss: 0.0233 - val_acc: 0.9978\n",
      "Epoch 123/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9943Epoch 00122: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0564 - acc: 0.9943 - val_loss: 0.0195 - val_acc: 0.9984\n",
      "Epoch 124/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9955Epoch 00123: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0384 - acc: 0.9955 - val_loss: 0.0171 - val_acc: 0.9984\n",
      "Epoch 125/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9944Epoch 00124: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0501 - acc: 0.9943 - val_loss: 0.0270 - val_acc: 0.9973\n",
      "Epoch 126/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9945Epoch 00125: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0567 - acc: 0.9945 - val_loss: 0.0183 - val_acc: 0.9975\n",
      "Epoch 127/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9949Epoch 00126: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0445 - acc: 0.9949 - val_loss: 0.0193 - val_acc: 0.9984\n",
      "Epoch 128/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9951Epoch 00127: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0514 - acc: 0.9951 - val_loss: 0.0197 - val_acc: 0.9984\n",
      "Epoch 129/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9944Epoch 00128: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0549 - acc: 0.9944 - val_loss: 0.0102 - val_acc: 0.9991\n",
      "Epoch 130/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9962Epoch 00129: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0340 - acc: 0.9962 - val_loss: 0.0172 - val_acc: 0.9984\n",
      "Epoch 131/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9943Epoch 00130: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0529 - acc: 0.9943 - val_loss: 0.0261 - val_acc: 0.9982\n",
      "Epoch 132/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9945Epoch 00131: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0616 - acc: 0.9945 - val_loss: 0.0127 - val_acc: 0.9991\n",
      "Epoch 133/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9954Epoch 00132: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0395 - acc: 0.9954 - val_loss: 0.0165 - val_acc: 0.9984\n",
      "Epoch 134/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9957Epoch 00133: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0452 - acc: 0.9957 - val_loss: 0.0119 - val_acc: 0.9991\n",
      "Epoch 135/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9953Epoch 00134: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0455 - acc: 0.9953 - val_loss: 0.0152 - val_acc: 0.9987\n",
      "Epoch 136/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9961Epoch 00135: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0393 - acc: 0.9960 - val_loss: 0.0228 - val_acc: 0.9984\n",
      "Epoch 137/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9969Epoch 00136: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0350 - acc: 0.9969 - val_loss: 0.0163 - val_acc: 0.9987\n",
      "Epoch 138/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9961Epoch 00137: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0397 - acc: 0.9961 - val_loss: 0.0277 - val_acc: 0.9978\n",
      "Epoch 139/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9949Epoch 00138: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0501 - acc: 0.9949 - val_loss: 0.0189 - val_acc: 0.9984\n",
      "Epoch 140/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9960Epoch 00139: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0398 - acc: 0.9960 - val_loss: 0.0151 - val_acc: 0.9989\n",
      "Epoch 141/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9954Epoch 00140: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0458 - acc: 0.9954 - val_loss: 0.0319 - val_acc: 0.9978\n",
      "Epoch 142/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9965Epoch 00141: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0335 - acc: 0.9965 - val_loss: 0.0324 - val_acc: 0.9973\n",
      "Epoch 143/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9968Epoch 00142: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0306 - acc: 0.9968 - val_loss: 0.0150 - val_acc: 0.9987\n",
      "Epoch 144/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9959Epoch 00143: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0447 - acc: 0.9959 - val_loss: 0.0249 - val_acc: 0.9984\n",
      "Epoch 145/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9961Epoch 00144: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0378 - acc: 0.9961 - val_loss: 0.0102 - val_acc: 0.9989\n",
      "Epoch 146/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9969Epoch 00145: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0274 - acc: 0.9969 - val_loss: 0.0211 - val_acc: 0.9987\n",
      "Epoch 147/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9951Epoch 00146: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0478 - acc: 0.9952 - val_loss: 0.0263 - val_acc: 0.9982\n",
      "Epoch 148/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9964Epoch 00147: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0372 - acc: 0.9964 - val_loss: 0.0178 - val_acc: 0.9984\n",
      "Epoch 149/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9950Epoch 00148: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0543 - acc: 0.9950 - val_loss: 0.0194 - val_acc: 0.9982\n",
      "Epoch 150/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9950Epoch 00149: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0460 - acc: 0.9950 - val_loss: 0.0235 - val_acc: 0.9978\n",
      "Epoch 151/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9955Epoch 00150: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0424 - acc: 0.9955 - val_loss: 0.0143 - val_acc: 0.9989\n",
      "Epoch 152/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9950Epoch 00151: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0467 - acc: 0.9950 - val_loss: 0.0191 - val_acc: 0.9982\n",
      "Epoch 153/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0234 - acc: 0.9973Epoch 00152: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0243 - acc: 0.9972 - val_loss: 0.0321 - val_acc: 0.9975\n",
      "Epoch 154/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9947Epoch 00153: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0579 - acc: 0.9947 - val_loss: 0.0243 - val_acc: 0.9980\n",
      "Epoch 155/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9953Epoch 00154: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0453 - acc: 0.9953 - val_loss: 0.0210 - val_acc: 0.9978\n",
      "Epoch 156/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9956Epoch 00155: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0410 - acc: 0.9956 - val_loss: 0.0150 - val_acc: 0.9989\n",
      "Epoch 157/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9961Epoch 00156: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0400 - acc: 0.9961 - val_loss: 0.0202 - val_acc: 0.9982\n",
      "Epoch 158/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9951Epoch 00157: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0479 - acc: 0.9952 - val_loss: 0.0242 - val_acc: 0.9982\n",
      "Epoch 159/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9953Epoch 00158: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0412 - acc: 0.9953 - val_loss: 0.0188 - val_acc: 0.9984\n",
      "Epoch 160/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9948Epoch 00159: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0507 - acc: 0.9948 - val_loss: 0.0229 - val_acc: 0.9978\n",
      "Epoch 161/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9964Epoch 00160: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0388 - acc: 0.9964 - val_loss: 0.0191 - val_acc: 0.9984\n",
      "Epoch 162/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9958Epoch 00161: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0421 - acc: 0.9958 - val_loss: 0.0171 - val_acc: 0.9987\n",
      "Epoch 163/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9949Epoch 00162: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0532 - acc: 0.9949 - val_loss: 0.0168 - val_acc: 0.9984\n",
      "Epoch 164/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9953Epoch 00163: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0522 - acc: 0.9953 - val_loss: 0.0129 - val_acc: 0.9987\n",
      "Epoch 165/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9954Epoch 00164: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0389 - acc: 0.9954 - val_loss: 0.0197 - val_acc: 0.9978\n",
      "Epoch 166/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9957Epoch 00165: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0389 - acc: 0.9957 - val_loss: 0.0220 - val_acc: 0.9984\n",
      "Epoch 167/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9961Epoch 00166: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0401 - acc: 0.9961 - val_loss: 0.0177 - val_acc: 0.9987\n",
      "Epoch 168/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9951Epoch 00167: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0475 - acc: 0.9952 - val_loss: 0.0147 - val_acc: 0.9987\n",
      "Epoch 169/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9972Epoch 00168: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0277 - acc: 0.9972 - val_loss: 0.0249 - val_acc: 0.9982\n",
      "Epoch 170/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9963Epoch 00169: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0355 - acc: 0.9963 - val_loss: 0.0205 - val_acc: 0.9987\n",
      "Epoch 171/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9959Epoch 00170: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0385 - acc: 0.9959 - val_loss: 0.0136 - val_acc: 0.9989\n",
      "Epoch 172/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9958Epoch 00171: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0405 - acc: 0.9958 - val_loss: 0.0204 - val_acc: 0.9980\n",
      "Epoch 173/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9970Epoch 00172: val_loss improved from 0.00922 to 0.00756, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.0276 - acc: 0.9970 - val_loss: 0.0076 - val_acc: 0.9993\n",
      "Epoch 174/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9975Epoch 00173: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0203 - acc: 0.9975 - val_loss: 0.0161 - val_acc: 0.9989\n",
      "Epoch 175/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9971Epoch 00174: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0230 - acc: 0.9971 - val_loss: 0.0158 - val_acc: 0.9987\n",
      "Epoch 176/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9964Epoch 00175: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0372 - acc: 0.9964 - val_loss: 0.0282 - val_acc: 0.9975\n",
      "Epoch 177/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9940Epoch 00176: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0662 - acc: 0.9939 - val_loss: 0.0234 - val_acc: 0.9980\n",
      "Epoch 178/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9946Epoch 00177: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0594 - acc: 0.9946 - val_loss: 0.0201 - val_acc: 0.9984\n",
      "Epoch 179/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9964Epoch 00178: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0311 - acc: 0.9964 - val_loss: 0.0192 - val_acc: 0.9987\n",
      "Epoch 180/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9962Epoch 00179: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0369 - acc: 0.9962 - val_loss: 0.0171 - val_acc: 0.9989\n",
      "Epoch 181/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9970Epoch 00180: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0250 - acc: 0.9970 - val_loss: 0.0196 - val_acc: 0.9984\n",
      "Epoch 182/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9969Epoch 00181: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0302 - acc: 0.9969 - val_loss: 0.0202 - val_acc: 0.9984\n",
      "Epoch 183/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9979Epoch 00182: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0181 - acc: 0.9979 - val_loss: 0.0190 - val_acc: 0.9982\n",
      "Epoch 184/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9971Epoch 00183: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0265 - acc: 0.9971 - val_loss: 0.0187 - val_acc: 0.9982\n",
      "Epoch 185/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9975Epoch 00184: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0251 - acc: 0.9975 - val_loss: 0.0200 - val_acc: 0.9980\n",
      "Epoch 186/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9973Epoch 00185: val_loss did not improve\n",
      "17939/17939 [==============================] - 20s - loss: 0.0290 - acc: 0.9973 - val_loss: 0.0205 - val_acc: 0.9984\n",
      "Epoch 187/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9968Epoch 00186: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0304 - acc: 0.9968 - val_loss: 0.0148 - val_acc: 0.9989\n",
      "Epoch 188/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9969Epoch 00187: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0314 - acc: 0.9969 - val_loss: 0.0151 - val_acc: 0.9989\n",
      "Epoch 189/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9972Epoch 00188: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0289 - acc: 0.9971 - val_loss: 0.0205 - val_acc: 0.9982\n",
      "Epoch 190/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9964Epoch 00189: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0358 - acc: 0.9964 - val_loss: 0.0282 - val_acc: 0.9975\n",
      "Epoch 191/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9971Epoch 00190: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0261 - acc: 0.9970 - val_loss: 0.0276 - val_acc: 0.9975\n",
      "Epoch 192/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9961Epoch 00191: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0378 - acc: 0.9961 - val_loss: 0.0149 - val_acc: 0.9989\n",
      "Epoch 193/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9964Epoch 00192: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0383 - acc: 0.9964 - val_loss: 0.0235 - val_acc: 0.9984\n",
      "Epoch 194/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9971Epoch 00193: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0300 - acc: 0.9971 - val_loss: 0.0099 - val_acc: 0.9993\n",
      "Epoch 195/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9982Epoch 00194: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0162 - acc: 0.9982 - val_loss: 0.0218 - val_acc: 0.9984\n",
      "Epoch 196/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9972Epoch 00195: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0278 - acc: 0.9972 - val_loss: 0.0150 - val_acc: 0.9989\n",
      "Epoch 197/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9963Epoch 00196: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0387 - acc: 0.9963 - val_loss: 0.0149 - val_acc: 0.9984\n",
      "Epoch 198/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9956Epoch 00197: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0470 - acc: 0.9956 - val_loss: 0.0218 - val_acc: 0.9982\n",
      "Epoch 199/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9957Epoch 00198: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0447 - acc: 0.9957 - val_loss: 0.0180 - val_acc: 0.9989\n",
      "Epoch 200/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9965Epoch 00199: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0348 - acc: 0.9965 - val_loss: 0.0115 - val_acc: 0.9987\n",
      "Epoch 201/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9972Epoch 00200: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0293 - acc: 0.9972 - val_loss: 0.0232 - val_acc: 0.9984\n",
      "Epoch 202/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9975Epoch 00201: val_loss improved from 0.00756 to 0.00749, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.0222 - acc: 0.9975 - val_loss: 0.0075 - val_acc: 0.9993\n",
      "Epoch 203/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9974Epoch 00202: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0225 - acc: 0.9974 - val_loss: 0.0178 - val_acc: 0.9989\n",
      "Epoch 204/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9969Epoch 00203: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0321 - acc: 0.9969 - val_loss: 0.0144 - val_acc: 0.9987\n",
      "Epoch 205/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9961Epoch 00204: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0389 - acc: 0.9962 - val_loss: 0.0109 - val_acc: 0.9989\n",
      "Epoch 206/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9968Epoch 00205: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0330 - acc: 0.9968 - val_loss: 0.0226 - val_acc: 0.9984\n",
      "Epoch 207/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9965Epoch 00206: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0336 - acc: 0.9965 - val_loss: 0.0117 - val_acc: 0.9989\n",
      "Epoch 208/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9961Epoch 00207: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0385 - acc: 0.9962 - val_loss: 0.0089 - val_acc: 0.9993\n",
      "Epoch 209/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9966Epoch 00208: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0410 - acc: 0.9967 - val_loss: 0.0169 - val_acc: 0.9987\n",
      "Epoch 210/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9967Epoch 00209: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0343 - acc: 0.9967 - val_loss: 0.0387 - val_acc: 0.9973\n",
      "Epoch 211/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9975Epoch 00210: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0289 - acc: 0.9975 - val_loss: 0.0272 - val_acc: 0.9978\n",
      "Epoch 212/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9965Epoch 00211: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0360 - acc: 0.9965 - val_loss: 0.0151 - val_acc: 0.9987\n",
      "Epoch 213/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9973Epoch 00212: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0227 - acc: 0.9973 - val_loss: 0.0141 - val_acc: 0.9987\n",
      "Epoch 214/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9973Epoch 00213: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0295 - acc: 0.9973 - val_loss: 0.0152 - val_acc: 0.9984\n",
      "Epoch 215/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9975Epoch 00214: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0238 - acc: 0.9975 - val_loss: 0.0108 - val_acc: 0.9991\n",
      "Epoch 216/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9973Epoch 00215: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0283 - acc: 0.9972 - val_loss: 0.0385 - val_acc: 0.9973\n",
      "Epoch 217/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9968Epoch 00216: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0302 - acc: 0.9968 - val_loss: 0.0138 - val_acc: 0.9989\n",
      "Epoch 218/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9962Epoch 00217: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0428 - acc: 0.9962 - val_loss: 0.0187 - val_acc: 0.9984\n",
      "Epoch 219/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9963Epoch 00218: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0400 - acc: 0.9963 - val_loss: 0.0267 - val_acc: 0.9978\n",
      "Epoch 220/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9980Epoch 00219: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0186 - acc: 0.9980 - val_loss: 0.0097 - val_acc: 0.9993\n",
      "Epoch 221/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9963Epoch 00220: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0369 - acc: 0.9963 - val_loss: 0.0172 - val_acc: 0.9987\n",
      "Epoch 222/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9968Epoch 00221: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0312 - acc: 0.9968 - val_loss: 0.0140 - val_acc: 0.9987\n",
      "Epoch 223/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9972Epoch 00222: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0257 - acc: 0.9972 - val_loss: 0.0171 - val_acc: 0.9989\n",
      "Epoch 224/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9971Epoch 00223: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0294 - acc: 0.9970 - val_loss: 0.0108 - val_acc: 0.9993\n",
      "Epoch 225/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9970Epoch 00224: val_loss did not improve\n",
      "17939/17939 [==============================] - 20s - loss: 0.0306 - acc: 0.9970 - val_loss: 0.0116 - val_acc: 0.9989\n",
      "Epoch 226/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0260 - acc: 0.9972Epoch 00225: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0260 - acc: 0.9972 - val_loss: 0.0163 - val_acc: 0.9984\n",
      "Epoch 227/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9974Epoch 00226: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0231 - acc: 0.9974 - val_loss: 0.0114 - val_acc: 0.9989\n",
      "Epoch 228/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9972Epoch 00227: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0311 - acc: 0.9972 - val_loss: 0.0186 - val_acc: 0.9987\n",
      "Epoch 229/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9977Epoch 00228: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0192 - acc: 0.9977 - val_loss: 0.0247 - val_acc: 0.9980\n",
      "Epoch 230/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9972Epoch 00229: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0308 - acc: 0.9972 - val_loss: 0.0108 - val_acc: 0.9993\n",
      "Epoch 231/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9972Epoch 00230: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0253 - acc: 0.9972 - val_loss: 0.0213 - val_acc: 0.9984\n",
      "Epoch 232/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9971Epoch 00231: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0258 - acc: 0.9971 - val_loss: 0.0178 - val_acc: 0.9984\n",
      "Epoch 233/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9968Epoch 00232: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0346 - acc: 0.9968 - val_loss: 0.0192 - val_acc: 0.9984\n",
      "Epoch 234/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9965Epoch 00233: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0291 - acc: 0.9965 - val_loss: 0.0129 - val_acc: 0.9989\n",
      "Epoch 235/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9983Epoch 00234: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0168 - acc: 0.9983 - val_loss: 0.0187 - val_acc: 0.9984\n",
      "Epoch 236/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9969Epoch 00235: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0286 - acc: 0.9969 - val_loss: 0.0118 - val_acc: 0.9991\n",
      "Epoch 237/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9968Epoch 00236: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0299 - acc: 0.9968 - val_loss: 0.0113 - val_acc: 0.9989\n",
      "Epoch 238/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9977Epoch 00237: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0226 - acc: 0.9977 - val_loss: 0.0199 - val_acc: 0.9982\n",
      "Epoch 239/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9959Epoch 00238: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0427 - acc: 0.9959 - val_loss: 0.0132 - val_acc: 0.9989\n",
      "Epoch 240/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9972Epoch 00239: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0269 - acc: 0.9972 - val_loss: 0.0213 - val_acc: 0.9984\n",
      "Epoch 241/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0221 - acc: 0.9973Epoch 00240: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0221 - acc: 0.9973 - val_loss: 0.0182 - val_acc: 0.9984\n",
      "Epoch 242/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9973Epoch 00241: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0229 - acc: 0.9973 - val_loss: 0.0264 - val_acc: 0.9982\n",
      "Epoch 243/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9971Epoch 00242: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0277 - acc: 0.9971 - val_loss: 0.0164 - val_acc: 0.9984\n",
      "Epoch 244/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9970Epoch 00243: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0300 - acc: 0.9970 - val_loss: 0.0235 - val_acc: 0.9984\n",
      "Epoch 245/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0210 - acc: 0.9976Epoch 00244: val_loss did not improve\n",
      "17939/17939 [==============================] - 20s - loss: 0.0210 - acc: 0.9976 - val_loss: 0.0155 - val_acc: 0.9987\n",
      "Epoch 246/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9977Epoch 00245: val_loss did not improve\n",
      "17939/17939 [==============================] - 20s - loss: 0.0200 - acc: 0.9977 - val_loss: 0.0134 - val_acc: 0.9991\n",
      "Epoch 247/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9970Epoch 00246: val_loss did not improve\n",
      "17939/17939 [==============================] - 20s - loss: 0.0257 - acc: 0.9970 - val_loss: 0.0165 - val_acc: 0.9987\n",
      "Epoch 248/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9973Epoch 00247: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0273 - acc: 0.9973 - val_loss: 0.0220 - val_acc: 0.9982\n",
      "Epoch 249/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9973Epoch 00248: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0274 - acc: 0.9973 - val_loss: 0.0200 - val_acc: 0.9987\n",
      "Epoch 250/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9972Epoch 00249: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0257 - acc: 0.9972 - val_loss: 0.0243 - val_acc: 0.9982\n",
      "Epoch 251/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9964Epoch 00250: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0318 - acc: 0.9964 - val_loss: 0.0104 - val_acc: 0.9989\n",
      "Epoch 252/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0190 - acc: 0.9977Epoch 00251: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0190 - acc: 0.9977 - val_loss: 0.0206 - val_acc: 0.9982\n",
      "Epoch 253/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9968Epoch 00252: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0311 - acc: 0.9968 - val_loss: 0.0114 - val_acc: 0.9989\n",
      "Epoch 254/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9981Epoch 00253: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0199 - acc: 0.9981 - val_loss: 0.0187 - val_acc: 0.9984\n",
      "Epoch 255/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9974Epoch 00254: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0257 - acc: 0.9974 - val_loss: 0.0109 - val_acc: 0.9991\n",
      "Epoch 256/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9967Epoch 00255: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0284 - acc: 0.9967 - val_loss: 0.0145 - val_acc: 0.9987\n",
      "Epoch 257/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9973Epoch 00256: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0282 - acc: 0.9973 - val_loss: 0.0238 - val_acc: 0.9984\n",
      "Epoch 258/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9968Epoch 00257: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0334 - acc: 0.9968 - val_loss: 0.0120 - val_acc: 0.9991\n",
      "Epoch 259/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9975Epoch 00258: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0201 - acc: 0.9975 - val_loss: 0.0196 - val_acc: 0.9987\n",
      "Epoch 260/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9974Epoch 00259: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0281 - acc: 0.9974 - val_loss: 0.0208 - val_acc: 0.9984\n",
      "Epoch 261/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9975Epoch 00260: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0237 - acc: 0.9975 - val_loss: 0.0127 - val_acc: 0.9989\n",
      "Epoch 262/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9974Epoch 00261: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0278 - acc: 0.9974 - val_loss: 0.0157 - val_acc: 0.9984\n",
      "Epoch 263/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9961Epoch 00262: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0408 - acc: 0.9962 - val_loss: 0.0144 - val_acc: 0.9991\n",
      "Epoch 264/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9973Epoch 00263: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0274 - acc: 0.9972 - val_loss: 0.0193 - val_acc: 0.9984\n",
      "Epoch 265/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9971Epoch 00264: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0214 - acc: 0.9971 - val_loss: 0.0181 - val_acc: 0.9982\n",
      "Epoch 266/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9972Epoch 00265: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0255 - acc: 0.9972 - val_loss: 0.0233 - val_acc: 0.9980\n",
      "Epoch 267/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9972Epoch 00266: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0246 - acc: 0.9972 - val_loss: 0.0202 - val_acc: 0.9987\n",
      "Epoch 268/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9973Epoch 00267: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0252 - acc: 0.9973 - val_loss: 0.0193 - val_acc: 0.9982\n",
      "Epoch 269/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9973Epoch 00268: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0264 - acc: 0.9973 - val_loss: 0.0284 - val_acc: 0.9978\n",
      "Epoch 270/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9967Epoch 00269: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0339 - acc: 0.9967 - val_loss: 0.0236 - val_acc: 0.9984\n",
      "Epoch 271/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9966Epoch 00270: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0376 - acc: 0.9967 - val_loss: 0.0287 - val_acc: 0.9980\n",
      "Epoch 272/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9969Epoch 00271: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0268 - acc: 0.9969 - val_loss: 0.0185 - val_acc: 0.9987\n",
      "Epoch 273/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9965Epoch 00272: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0369 - acc: 0.9965 - val_loss: 0.0267 - val_acc: 0.9980\n",
      "Epoch 274/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9960Epoch 00273: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0413 - acc: 0.9960 - val_loss: 0.0183 - val_acc: 0.9982\n",
      "Epoch 275/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9963Epoch 00274: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0363 - acc: 0.9963 - val_loss: 0.0212 - val_acc: 0.9984\n",
      "Epoch 276/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9967Epoch 00275: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0301 - acc: 0.9967 - val_loss: 0.0339 - val_acc: 0.9975\n",
      "Epoch 277/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9974Epoch 00276: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0193 - acc: 0.9974 - val_loss: 0.0169 - val_acc: 0.9984\n",
      "Epoch 278/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9969Epoch 00277: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0257 - acc: 0.9969 - val_loss: 0.0107 - val_acc: 0.9989\n",
      "Epoch 279/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9973Epoch 00278: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0263 - acc: 0.9973 - val_loss: 0.0227 - val_acc: 0.9982\n",
      "Epoch 280/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9985Epoch 00279: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0139 - acc: 0.9986 - val_loss: 0.0194 - val_acc: 0.9984\n",
      "Epoch 281/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9972Epoch 00280: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0230 - acc: 0.9972 - val_loss: 0.0232 - val_acc: 0.9984\n",
      "Epoch 282/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9967Epoch 00281: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0335 - acc: 0.9967 - val_loss: 0.0210 - val_acc: 0.9982\n",
      "Epoch 283/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9979Epoch 00282: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0170 - acc: 0.9979 - val_loss: 0.0224 - val_acc: 0.9984\n",
      "Epoch 284/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9978Epoch 00283: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0178 - acc: 0.9978 - val_loss: 0.0255 - val_acc: 0.9978\n",
      "Epoch 285/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9974Epoch 00284: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0244 - acc: 0.9974 - val_loss: 0.0414 - val_acc: 0.9969\n",
      "Epoch 286/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9973Epoch 00285: val_loss did not improve\n",
      "17939/17939 [==============================] - 20s - loss: 0.0203 - acc: 0.9973 - val_loss: 0.0184 - val_acc: 0.9984\n",
      "Epoch 287/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9979Epoch 00286: val_loss did not improve\n",
      "17939/17939 [==============================] - 20s - loss: 0.0151 - acc: 0.9979 - val_loss: 0.0236 - val_acc: 0.9984\n",
      "Epoch 288/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9973Epoch 00287: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0260 - acc: 0.9973 - val_loss: 0.0215 - val_acc: 0.9984\n",
      "Epoch 289/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9978Epoch 00288: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0199 - acc: 0.9978 - val_loss: 0.0188 - val_acc: 0.9982\n",
      "Epoch 290/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9973Epoch 00289: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0256 - acc: 0.9973 - val_loss: 0.0188 - val_acc: 0.9987\n",
      "Epoch 291/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9973Epoch 00290: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0282 - acc: 0.9973 - val_loss: 0.0165 - val_acc: 0.9987\n",
      "Epoch 292/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9979Epoch 00291: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0174 - acc: 0.9979 - val_loss: 0.0243 - val_acc: 0.9978\n",
      "Epoch 293/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0219 - acc: 0.9978Epoch 00292: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0219 - acc: 0.9978 - val_loss: 0.0149 - val_acc: 0.9989\n",
      "Epoch 294/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9980Epoch 00293: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0198 - acc: 0.9980 - val_loss: 0.0146 - val_acc: 0.9984\n",
      "Epoch 295/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9965Epoch 00294: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0342 - acc: 0.9965 - val_loss: 0.0209 - val_acc: 0.9984\n",
      "Epoch 296/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9968Epoch 00295: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0313 - acc: 0.9968 - val_loss: 0.0114 - val_acc: 0.9989\n",
      "Epoch 297/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9985Epoch 00296: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0142 - acc: 0.9985 - val_loss: 0.0143 - val_acc: 0.9987\n",
      "Epoch 298/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9986Epoch 00297: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0129 - acc: 0.9986 - val_loss: 0.0235 - val_acc: 0.9980\n",
      "Epoch 299/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9967Epoch 00298: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0292 - acc: 0.9967 - val_loss: 0.0180 - val_acc: 0.9987\n",
      "Epoch 300/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9982Epoch 00299: val_loss did not improve\n",
      "17939/17939 [==============================] - 20s - loss: 0.0125 - acc: 0.9982 - val_loss: 0.0188 - val_acc: 0.9984\n",
      "Epoch 301/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9983Epoch 00300: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0141 - acc: 0.9983 - val_loss: 0.0185 - val_acc: 0.9984\n",
      "Epoch 302/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9986Epoch 00301: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0128 - acc: 0.9986 - val_loss: 0.0155 - val_acc: 0.9989\n",
      "Epoch 303/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9978Epoch 00302: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0202 - acc: 0.9978 - val_loss: 0.0253 - val_acc: 0.9982\n",
      "Epoch 304/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9980Epoch 00303: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0198 - acc: 0.9980 - val_loss: 0.0165 - val_acc: 0.9989\n",
      "Epoch 305/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9973Epoch 00304: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0193 - acc: 0.9973 - val_loss: 0.0137 - val_acc: 0.9987\n",
      "Epoch 306/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9977Epoch 00305: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0250 - acc: 0.9977 - val_loss: 0.0252 - val_acc: 0.9982\n",
      "Epoch 307/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9973Epoch 00306: val_loss improved from 0.00749 to 0.00719, saving model to saved_models/weights.best.VGG16.hdf5_transfer_learning2\n",
      "17939/17939 [==============================] - 21s - loss: 0.0251 - acc: 0.9973 - val_loss: 0.0072 - val_acc: 0.9996\n",
      "Epoch 308/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9972Epoch 00307: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0278 - acc: 0.9972 - val_loss: 0.0094 - val_acc: 0.9991\n",
      "Epoch 309/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9977Epoch 00308: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0205 - acc: 0.9977 - val_loss: 0.0174 - val_acc: 0.9989\n",
      "Epoch 310/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9973Epoch 00309: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0261 - acc: 0.9973 - val_loss: 0.0179 - val_acc: 0.9989\n",
      "Epoch 311/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9974Epoch 00310: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0292 - acc: 0.9974 - val_loss: 0.0139 - val_acc: 0.9987\n",
      "Epoch 312/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9976Epoch 00311: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0217 - acc: 0.9976 - val_loss: 0.0245 - val_acc: 0.9982\n",
      "Epoch 313/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9977Epoch 00312: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0192 - acc: 0.9977 - val_loss: 0.0095 - val_acc: 0.9991\n",
      "Epoch 314/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9982Epoch 00313: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0163 - acc: 0.9982 - val_loss: 0.0190 - val_acc: 0.9984\n",
      "Epoch 315/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9983Epoch 00314: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0149 - acc: 0.9983 - val_loss: 0.0121 - val_acc: 0.9989\n",
      "Epoch 316/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9978Epoch 00315: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0182 - acc: 0.9978 - val_loss: 0.0144 - val_acc: 0.9991\n",
      "Epoch 317/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9972Epoch 00316: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0252 - acc: 0.9972 - val_loss: 0.0133 - val_acc: 0.9989\n",
      "Epoch 318/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9971Epoch 00317: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0264 - acc: 0.9971 - val_loss: 0.0138 - val_acc: 0.9989\n",
      "Epoch 319/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9981Epoch 00318: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0135 - acc: 0.9981 - val_loss: 0.0144 - val_acc: 0.9991\n",
      "Epoch 320/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9978Epoch 00319: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0215 - acc: 0.9978 - val_loss: 0.0097 - val_acc: 0.9989\n",
      "Epoch 321/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0233 - acc: 0.9974Epoch 00320: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0232 - acc: 0.9974 - val_loss: 0.0138 - val_acc: 0.9989\n",
      "Epoch 322/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.9975Epoch 00321: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0213 - acc: 0.9975 - val_loss: 0.0137 - val_acc: 0.9989\n",
      "Epoch 323/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9981Epoch 00322: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0191 - acc: 0.9981 - val_loss: 0.0133 - val_acc: 0.9989\n",
      "Epoch 324/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9983Epoch 00323: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0157 - acc: 0.9983 - val_loss: 0.0114 - val_acc: 0.9989\n",
      "Epoch 325/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9981Epoch 00324: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0156 - acc: 0.9981 - val_loss: 0.0130 - val_acc: 0.9989\n",
      "Epoch 326/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9985Epoch 00325: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0134 - acc: 0.9985 - val_loss: 0.0178 - val_acc: 0.9987\n",
      "Epoch 327/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9983Epoch 00326: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0159 - acc: 0.9983 - val_loss: 0.0182 - val_acc: 0.9984\n",
      "Epoch 328/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9985Epoch 00327: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0136 - acc: 0.9985 - val_loss: 0.0233 - val_acc: 0.9982\n",
      "Epoch 329/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9982Epoch 00328: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0176 - acc: 0.9982 - val_loss: 0.0236 - val_acc: 0.9980\n",
      "Epoch 330/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9982Epoch 00329: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0173 - acc: 0.9982 - val_loss: 0.0163 - val_acc: 0.9987\n",
      "Epoch 331/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9984Epoch 00330: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0169 - acc: 0.9984 - val_loss: 0.0172 - val_acc: 0.9989\n",
      "Epoch 332/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9981Epoch 00331: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0170 - acc: 0.9981 - val_loss: 0.0240 - val_acc: 0.9980\n",
      "Epoch 333/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9982Epoch 00332: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0158 - acc: 0.9982 - val_loss: 0.0165 - val_acc: 0.9984\n",
      "Epoch 334/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0203 - acc: 0.9980Epoch 00333: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0202 - acc: 0.9980 - val_loss: 0.0227 - val_acc: 0.9984\n",
      "Epoch 335/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9982Epoch 00334: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0155 - acc: 0.9982 - val_loss: 0.0186 - val_acc: 0.9987\n",
      "Epoch 336/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9988Epoch 00335: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0141 - acc: 0.9988 - val_loss: 0.0164 - val_acc: 0.9984\n",
      "Epoch 337/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9986Epoch 00336: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0138 - acc: 0.9986 - val_loss: 0.0121 - val_acc: 0.9991\n",
      "Epoch 338/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9983Epoch 00337: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0120 - acc: 0.9983 - val_loss: 0.0221 - val_acc: 0.9984\n",
      "Epoch 339/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9966Epoch 00338: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0342 - acc: 0.9966 - val_loss: 0.0110 - val_acc: 0.9991\n",
      "Epoch 340/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9978Epoch 00339: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0232 - acc: 0.9978 - val_loss: 0.0178 - val_acc: 0.9984\n",
      "Epoch 341/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9969Epoch 00340: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0287 - acc: 0.9969 - val_loss: 0.0184 - val_acc: 0.9984\n",
      "Epoch 342/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9974Epoch 00341: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0225 - acc: 0.9974 - val_loss: 0.0217 - val_acc: 0.9982\n",
      "Epoch 343/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9978Epoch 00342: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0180 - acc: 0.9978 - val_loss: 0.0237 - val_acc: 0.9978\n",
      "Epoch 344/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9967Epoch 00343: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0295 - acc: 0.9967 - val_loss: 0.0082 - val_acc: 0.9993\n",
      "Epoch 345/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0266 - acc: 0.9977Epoch 00344: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0265 - acc: 0.9977 - val_loss: 0.0223 - val_acc: 0.9978\n",
      "Epoch 346/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0247 - acc: 0.9974Epoch 00345: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0247 - acc: 0.9974 - val_loss: 0.0105 - val_acc: 0.9991\n",
      "Epoch 347/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9984Epoch 00346: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0148 - acc: 0.9984 - val_loss: 0.0149 - val_acc: 0.9987\n",
      "Epoch 348/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9985Epoch 00347: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0151 - acc: 0.9986 - val_loss: 0.0181 - val_acc: 0.9984\n",
      "Epoch 349/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9982Epoch 00348: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0163 - acc: 0.9982 - val_loss: 0.0110 - val_acc: 0.9991\n",
      "Epoch 350/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9982Epoch 00349: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0186 - acc: 0.9982 - val_loss: 0.0104 - val_acc: 0.9991\n",
      "Epoch 351/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9975Epoch 00350: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0243 - acc: 0.9975 - val_loss: 0.0103 - val_acc: 0.9991\n",
      "Epoch 352/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9987Epoch 00351: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0115 - acc: 0.9987 - val_loss: 0.0202 - val_acc: 0.9984\n",
      "Epoch 353/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9977Epoch 00352: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0190 - acc: 0.9977 - val_loss: 0.0107 - val_acc: 0.9991\n",
      "Epoch 354/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0115 - acc: 0.9988Epoch 00353: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0115 - acc: 0.9988 - val_loss: 0.0155 - val_acc: 0.9987\n",
      "Epoch 355/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9979Epoch 00354: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0136 - acc: 0.9979 - val_loss: 0.0134 - val_acc: 0.9989\n",
      "Epoch 356/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9978Epoch 00355: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0193 - acc: 0.9978 - val_loss: 0.0207 - val_acc: 0.9987\n",
      "Epoch 357/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9979Epoch 00356: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0177 - acc: 0.9979 - val_loss: 0.0073 - val_acc: 0.9996\n",
      "Epoch 358/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9987Epoch 00357: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0131 - acc: 0.9987 - val_loss: 0.0198 - val_acc: 0.9984\n",
      "Epoch 359/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9978Epoch 00358: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0161 - acc: 0.9978 - val_loss: 0.0158 - val_acc: 0.9989\n",
      "Epoch 360/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9988Epoch 00359: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0143 - acc: 0.9988 - val_loss: 0.0073 - val_acc: 0.9996\n",
      "Epoch 361/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9984Epoch 00360: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0136 - acc: 0.9984 - val_loss: 0.0224 - val_acc: 0.9982\n",
      "Epoch 362/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9984Epoch 00361: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0186 - acc: 0.9984 - val_loss: 0.0181 - val_acc: 0.9987\n",
      "Epoch 363/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9983Epoch 00362: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0131 - acc: 0.9983 - val_loss: 0.0153 - val_acc: 0.9987\n",
      "Epoch 364/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9982Epoch 00363: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0153 - acc: 0.9982 - val_loss: 0.0073 - val_acc: 0.9996\n",
      "Epoch 365/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9984Epoch 00364: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0158 - acc: 0.9984 - val_loss: 0.0090 - val_acc: 0.9991\n",
      "Epoch 366/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9980Epoch 00365: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0197 - acc: 0.9980 - val_loss: 0.0287 - val_acc: 0.9978\n",
      "Epoch 367/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9978Epoch 00366: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0238 - acc: 0.9978 - val_loss: 0.0123 - val_acc: 0.9991\n",
      "Epoch 368/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9983Epoch 00367: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0171 - acc: 0.9983 - val_loss: 0.0202 - val_acc: 0.9984\n",
      "Epoch 369/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9983Epoch 00368: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0186 - acc: 0.9983 - val_loss: 0.0126 - val_acc: 0.9991\n",
      "Epoch 370/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9987Epoch 00369: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0154 - acc: 0.9987 - val_loss: 0.0179 - val_acc: 0.9987\n",
      "Epoch 371/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9989Epoch 00370: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0113 - acc: 0.9989 - val_loss: 0.0190 - val_acc: 0.9980\n",
      "Epoch 372/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9984Epoch 00371: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0145 - acc: 0.9984 - val_loss: 0.0096 - val_acc: 0.9989\n",
      "Epoch 373/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9979Epoch 00372: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0182 - acc: 0.9979 - val_loss: 0.0141 - val_acc: 0.9987\n",
      "Epoch 374/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9982Epoch 00373: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0175 - acc: 0.9981 - val_loss: 0.0116 - val_acc: 0.9989\n",
      "Epoch 375/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9977Epoch 00374: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0211 - acc: 0.9977 - val_loss: 0.0174 - val_acc: 0.9987\n",
      "Epoch 376/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9987Epoch 00375: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0138 - acc: 0.9987 - val_loss: 0.0092 - val_acc: 0.9989\n",
      "Epoch 377/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0094 - acc: 0.9989Epoch 00376: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0094 - acc: 0.9989 - val_loss: 0.0108 - val_acc: 0.9989\n",
      "Epoch 378/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9985Epoch 00377: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0171 - acc: 0.9985 - val_loss: 0.0133 - val_acc: 0.9989\n",
      "Epoch 379/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9986Epoch 00378: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0146 - acc: 0.9986 - val_loss: 0.0108 - val_acc: 0.9991\n",
      "Epoch 380/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9983Epoch 00379: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0120 - acc: 0.9983 - val_loss: 0.0134 - val_acc: 0.9991\n",
      "Epoch 381/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0111 - acc: 0.9988Epoch 00380: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0111 - acc: 0.9988 - val_loss: 0.0219 - val_acc: 0.9982\n",
      "Epoch 382/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9982Epoch 00381: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0196 - acc: 0.9982 - val_loss: 0.0113 - val_acc: 0.9991\n",
      "Epoch 383/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0067 - acc: 0.9990Epoch 00382: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0076 - acc: 0.9989 - val_loss: 0.0086 - val_acc: 0.9991\n",
      "Epoch 384/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9977Epoch 00383: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0248 - acc: 0.9977 - val_loss: 0.0144 - val_acc: 0.9991\n",
      "Epoch 385/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0112 - acc: 0.9986Epoch 00384: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0112 - acc: 0.9986 - val_loss: 0.0185 - val_acc: 0.9987\n",
      "Epoch 386/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9978Epoch 00385: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0239 - acc: 0.9978 - val_loss: 0.0154 - val_acc: 0.9984\n",
      "Epoch 387/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9980Epoch 00386: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0170 - acc: 0.9980 - val_loss: 0.0081 - val_acc: 0.9991\n",
      "Epoch 388/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9979Epoch 00387: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0248 - acc: 0.9979 - val_loss: 0.0146 - val_acc: 0.9991\n",
      "Epoch 389/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9980Epoch 00388: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0163 - acc: 0.9980 - val_loss: 0.0119 - val_acc: 0.9987\n",
      "Epoch 390/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0235 - acc: 0.9976Epoch 00389: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0235 - acc: 0.9976 - val_loss: 0.0198 - val_acc: 0.9984\n",
      "Epoch 391/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0218 - acc: 0.9977Epoch 00390: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0218 - acc: 0.9977 - val_loss: 0.0113 - val_acc: 0.9991\n",
      "Epoch 392/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9978Epoch 00391: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0199 - acc: 0.9978 - val_loss: 0.0206 - val_acc: 0.9987\n",
      "Epoch 393/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9984Epoch 00392: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0134 - acc: 0.9984 - val_loss: 0.0244 - val_acc: 0.9980\n",
      "Epoch 394/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9985Epoch 00393: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0113 - acc: 0.9985 - val_loss: 0.0198 - val_acc: 0.9987\n",
      "Epoch 395/400\n",
      "17904/17939 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9985Epoch 00394: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0157 - acc: 0.9985 - val_loss: 0.0172 - val_acc: 0.9987\n",
      "Epoch 396/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9983Epoch 00395: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0141 - acc: 0.9983 - val_loss: 0.0157 - val_acc: 0.9987\n",
      "Epoch 397/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0109 - acc: 0.9987Epoch 00396: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0109 - acc: 0.9987 - val_loss: 0.0169 - val_acc: 0.9982\n",
      "Epoch 398/400\n",
      "17920/17939 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9984Epoch 00397: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0145 - acc: 0.9984 - val_loss: 0.0274 - val_acc: 0.9980\n",
      "Epoch 399/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9990Epoch 00398: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0103 - acc: 0.9990 - val_loss: 0.0126 - val_acc: 0.9989\n",
      "Epoch 400/400\n",
      "17936/17939 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9984Epoch 00399: val_loss did not improve\n",
      "17939/17939 [==============================] - 21s - loss: 0.0160 - acc: 0.9984 - val_loss: 0.0072 - val_acc: 0.9996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7adc1dff98>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5_transfer_learning2', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model2.fit(bottleneck_features_train2_VGG16, train_targets, \n",
    "          validation_data=(bottleneck_features_valid2_VGG16, valid_targets),\n",
    "          epochs=400, batch_size=16, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model2.load_weights('saved_models/weights.best.VGG16.hdf5_transfer_learning2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "The model is tried on the test dataset of driver images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_predictions2 = [VGG16_model2.predict(np.expand_dims(tensor, axis=0))[0] for tensor in bottleneck_features_test2_VGG16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_subm2 = np.column_stack((np.asarray(test_files_final), np.asarray(VGG16_predictions2,dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Train a CNN with Transfer Learning - Part2\n",
    "\n",
    "To improve accuracy, a CNN is trained using transfer learning.  \n",
    "\n",
    "### Fine-tuning the top layers of a a pre-trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Instantiating the VGG16 base for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "base_model = VGG16(include_top=False)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each driver category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 5,130\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aind2/anaconda3/envs/aind2/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "VGG16_top_model = Sequential()\n",
    "VGG16_top_model.add(GlobalAveragePooling2D(input_shape=base_model.output_shape[1:]))\n",
    "VGG16_top_model.add(Dense(10, activation='softmax', kernel_initializer='glorot_normal'))\n",
    "\n",
    "\n",
    "VGG16_top_model.load_weights('saved_models/weights.best.VGG16.hdf5_transfer_learning')\n",
    "\n",
    "model = Model(input= base_model.input, output= VGG16_top_model(base_model.output))\n",
    "\n",
    "VGG16_top_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model\n",
    "\n",
    "Upto 15 layers of the VGG16 model were frozen so that the last convolutional block and the fully connected layer which is added to the top can be fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the first 15 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "Fine-tuning is done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 16s 197ms/step - loss: 0.0706 - accuracy: 0.9750 - val_loss: 3.5052 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.50520, saving model to weights.best.VGG16.hdf5_fine_tuning\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 22s 279ms/step - loss: 0.1590 - accuracy: 0.9625 - val_loss: 7.7819 - val_accuracy: 0.3000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 3.50520\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 16s 204ms/step - loss: 0.5587 - accuracy: 0.8500 - val_loss: 2.8033 - val_accuracy: 0.4000\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.50520 to 2.80328, saving model to weights.best.VGG16.hdf5_fine_tuning\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 15s 184ms/step - loss: 0.1547 - accuracy: 0.9375 - val_loss: 2.4680 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.80328 to 2.46804, saving model to weights.best.VGG16.hdf5_fine_tuning\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 20s 256ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 2.9040 - val_accuracy: 0.4500\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.46804\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 24s 295ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 3.1479 - val_accuracy: 0.4500\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.46804\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 19s 232ms/step - loss: 0.0737 - accuracy: 0.9750 - val_loss: 3.6148 - val_accuracy: 0.3000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.46804\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 17s 213ms/step - loss: 0.4766 - accuracy: 0.8750 - val_loss: 1.8923 - val_accuracy: 0.4500\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.46804 to 1.89235, saving model to weights.best.VGG16.hdf5_fine_tuning\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 22s 276ms/step - loss: 0.2277 - accuracy: 0.9500 - val_loss: 2.2554 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.89235\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 23s 291ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 3.1048 - val_accuracy: 0.4500\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.89235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd69f2234a8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.VGG16.hdf5_fine_tuning', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=10, batch_size=16, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights.best.VGG16.hdf5_fine_tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "The model is tried on the test dataset of driver images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_predictions_fine_tuned = [model.predict(np.expand_dims(tensor, axis=0))[0] for tensor in test_tensors]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_files_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-197d6f038f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVGG16_subm_fine_tuned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVGG16_predictions_fine_tuned\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_files_final' is not defined"
     ]
    }
   ],
   "source": [
    "VGG16_subm_fine_tuned = np.column_stack((np.asarray(test_files_final), np.asarray(VGG16_predictions_fine_tuned,dtype=np.float32)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
